{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvzwcTlBdqc3eU+d9+x105",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kareemullah123456789/big_data_advanced/blob/main/advance_pyspark_unedited.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D_FW5UABtX3g",
        "outputId": "73b6a8e5-af8c-488b-9466-93a61c1e41dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting delta-spark\n",
            "  Downloading delta_spark-4.0.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-4.0.0.tar.gz (434.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m434.1/434.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from delta-spark) (8.7.0)\n",
            "Collecting py4j==0.10.9.9 (from pyspark)\n",
            "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.0.0->delta-spark) (3.23.0)\n",
            "Downloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Downloading delta_spark-4.0.0-py3-none-any.whl (39 kB)\n",
            "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-4.0.0-py2.py3-none-any.whl size=434741241 sha256=f1a6e5903fc25ed6e826e7d6747ea41e5decae500935016f75b4e2ea0906b44a\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/e4/c1/3c917d48563ae77204dd185aa3da90da6a1a5526565296dadf\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark, pyngrok, delta-spark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.2 requires pyspark[connect]~=3.5.1, but you have pyspark 4.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed delta-spark-4.0.0 py4j-0.10.9.9 pyngrok-7.2.12 pyspark-4.0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PySparkRuntimeError",
          "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-845185215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.execution.arrow.pyspark.enabled\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigure_spark_with_delta_pip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üöÄ Advanced PySpark Tutorial Started!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/core/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/core/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    203\u001b[0m             )\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/core/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 raise PySparkRuntimeError(\n\u001b[0m\u001b[1;32m    112\u001b[0m                     \u001b[0merrorClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"JAVA_GATEWAY_EXITED\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mmessageParameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
          ]
        }
      ],
      "source": [
        "# Advanced PySpark Tutorial: Performance Optimization\n",
        "# Topics: Partitioning, Joins, Caching, File Formats, Memory, Catalyst Optimizer\n",
        "\n",
        "!pip install pyspark pyngrok delta-spark\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark import StorageLevel\n",
        "from pyspark.sql.window import Window\n",
        "from delta import configure_spark_with_delta_pip\n",
        "import pandas as pd\n",
        "\n",
        "# Configure Spark with Delta Lake and optimization settings\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"AdvancedPySparkTutorial\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "\n",
        "print(\"üöÄ Advanced PySpark Tutorial Started!\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 1. PARTITIONING STRATEGIES üîÑ\n",
        "# =============================================================================\n",
        "\n",
        "def demo_partitioning_strategies():\n",
        "    \"\"\"Comprehensive partitioning demo with performance analysis\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üîÑ PARTITIONING STRATEGIES DEMO\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create sample datasets\n",
        "    print(\"üìä Creating sample datasets...\")\n",
        "\n",
        "    # Large dataset for partitioning tests\n",
        "    large_data = [(i, f\"user_{i%1000}\", i * 10, i % 50, f\"2023-{(i%12)+1:02d}-01\")\n",
        "                  for i in range(1, 500001)]  # 500K records\n",
        "\n",
        "    df_large = spark.createDataFrame(large_data,\n",
        "                                   [\"id\", \"user_name\", \"amount\", \"category\", \"date\"])\n",
        "\n",
        "    print(f\"‚úÖ Created dataset with {df_large.count():,} records\")\n",
        "    print(f\"üî¢ Current partitions: {df_large.rdd.getNumPartitions()}\")\n",
        "\n",
        "    # Measure performance for different partitioning strategies\n",
        "    def measure_operation(name, operation):\n",
        "        start = time.time()\n",
        "        result = operation()\n",
        "        duration = time.time() - start\n",
        "        print(f\"   {name}: {duration:.3f}s\")\n",
        "        return result, duration\n",
        "\n",
        "    print(\"\\nüéØ Testing Different Partitioning Strategies:\")\n",
        "\n",
        "    # 1. Default partitioning performance\n",
        "    print(\"\\n1Ô∏è‚É£ Default Partitioning:\")\n",
        "    _, time_default = measure_operation(\n",
        "        \"Filter + GroupBy\",\n",
        "        lambda: df_large.filter(col(\"amount\") > 1000000).groupBy(\"category\").count().count()\n",
        "    )\n",
        "\n",
        "    # 2. Repartition vs Coalesce\n",
        "    print(\"\\n2Ô∏è‚É£ Repartition vs Coalesce:\")\n",
        "\n",
        "    # Repartition (full shuffle)\n",
        "    df_repartitioned = df_large.repartition(20)\n",
        "    _, time_repartition = measure_operation(\n",
        "        \"After repartition(20)\",\n",
        "        lambda: df_repartitioned.filter(col(\"amount\") > 1000000).groupBy(\"category\").count().count()\n",
        "    )\n",
        "\n",
        "    # Coalesce (minimize shuffle)\n",
        "    df_coalesced = df_large.coalesce(8)\n",
        "    _, time_coalesce = measure_operation(\n",
        "        \"After coalesce(8)\",\n",
        "        lambda: df_coalesced.filter(col(\"amount\") > 1000000).groupBy(\"category\").count().count()\n",
        "    )\n",
        "\n",
        "    # 3. Hash Partitioning\n",
        "    print(\"\\n3Ô∏è‚É£ Hash Partitioning by Category:\")\n",
        "    df_hash_partitioned = df_large.repartition(col(\"category\"))\n",
        "    _, time_hash = measure_operation(\n",
        "        \"Hash partitioned GroupBy\",\n",
        "        lambda: df_hash_partitioned.groupBy(\"category\").count().count()\n",
        "    )\n",
        "\n",
        "    # 4. Range Partitioning\n",
        "    print(\"\\n4Ô∏è‚É£ Range Partitioning by ID:\")\n",
        "    df_range_partitioned = df_large.repartitionByRange(10, col(\"id\"))\n",
        "    _, time_range = measure_operation(\n",
        "        \"Range partitioned filter\",\n",
        "        lambda: df_range_partitioned.filter((col(\"id\") >= 100000) & (col(\"id\") <= 200000)).count()\n",
        "    )\n",
        "\n",
        "    # 5. Partition Skew Detection\n",
        "    print(\"\\n5Ô∏è‚É£ Partition Skew Analysis:\")\n",
        "\n",
        "    # Create skewed data\n",
        "    skewed_data = ([(i, f\"user_{i}\", i * 10, 1, \"2023-01-01\") for i in range(1, 400001)] +  # 80% in category 1\n",
        "                   [(i, f\"user_{i}\", i * 10, j, \"2023-01-01\") for i in range(400001, 500001) for j in range(2, 6)])  # 20% in categories 2-5\n",
        "\n",
        "    df_skewed = spark.createDataFrame(skewed_data, [\"id\", \"user_name\", \"amount\", \"category\", \"date\"])\n",
        "\n",
        "    # Analyze partition sizes\n",
        "    def analyze_partition_skew(df, name):\n",
        "        partition_sizes = df.rdd.mapPartitions(lambda x: [sum(1 for _ in x)]).collect()\n",
        "        print(f\"   {name} partition sizes: {partition_sizes}\")\n",
        "        print(f\"   Skew ratio: {max(partition_sizes) / min(partition_sizes) if min(partition_sizes) > 0 else 'inf':.2f}\")\n",
        "\n",
        "    analyze_partition_skew(df_skewed, \"Skewed data\")\n",
        "\n",
        "    # Fix skew with salting technique\n",
        "    df_salted = df_skewed.withColumn(\"salt\", (rand() * 10).cast(\"int\")) \\\n",
        "                         .repartition(col(\"category\"), col(\"salt\"))\n",
        "\n",
        "    analyze_partition_skew(df_salted, \"After salting\")\n",
        "\n",
        "    return {\n",
        "        'default': time_default,\n",
        "        'repartition': time_repartition,\n",
        "        'coalesce': time_coalesce,\n",
        "        'hash': time_hash,\n",
        "        'range': time_range\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# 2. JOIN OPTIMIZATIONS üîó\n",
        "# =============================================================================\n",
        "\n",
        "def demo_join_optimizations():\n",
        "    \"\"\"Comprehensive join optimization strategies\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üîó JOIN OPTIMIZATIONS DEMO\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create datasets for join demos\n",
        "    print(\"üìä Creating datasets for join optimization...\")\n",
        "\n",
        "    # Large fact table\n",
        "    fact_data = [(i, f\"product_{i%1000}\", i * 100, i % 50) for i in range(1, 100001)]\n",
        "    df_large = spark.createDataFrame(fact_data, [\"order_id\", \"product_id\", \"amount\", \"customer_id\"])\n",
        "\n",
        "    # Small dimension table (perfect for broadcast)\n",
        "    dim_data = [(i, f\"Customer_{i}\", f\"City_{i%10}\") for i in range(1, 51)]\n",
        "    df_small = spark.createDataFrame(dim_data, [\"customer_id\", \"customer_name\", \"city\"])\n",
        "\n",
        "    # Medium table for shuffle join demo\n",
        "    medium_data = [(i, f\"Product_{i}\", f\"Category_{i%20}\") for i in range(1, 1001)]\n",
        "    df_medium = spark.createDataFrame(medium_data, [\"product_id\", \"product_name\", \"category\"])\n",
        "\n",
        "    print(f\"‚úÖ Large table: {df_large.count():,} records\")\n",
        "    print(f\"‚úÖ Small table: {df_small.count():,} records\")\n",
        "    print(f\"‚úÖ Medium table: {df_medium.count():,} records\")\n",
        "\n",
        "    def measure_join(name, join_operation):\n",
        "        start = time.time()\n",
        "        result = join_operation().count()\n",
        "        duration = time.time() - start\n",
        "        print(f\"   {name}: {duration:.3f}s ({result:,} results)\")\n",
        "        return duration\n",
        "\n",
        "    print(\"\\nüéØ Testing Different Join Strategies:\")\n",
        "\n",
        "    # 1. Broadcast Join (automatic)\n",
        "    print(\"\\n1Ô∏è‚É£ Broadcast Join (Small Table):\")\n",
        "    time_broadcast = measure_join(\n",
        "        \"Auto broadcast join\",\n",
        "        lambda: df_large.join(df_small, \"customer_id\")\n",
        "    )\n",
        "\n",
        "    # 2. Explicit Broadcast Hint\n",
        "    print(\"\\n2Ô∏è‚É£ Explicit Broadcast Hint:\")\n",
        "    time_explicit_broadcast = measure_join(\n",
        "        \"Explicit broadcast hint\",\n",
        "        lambda: df_large.join(broadcast(df_small), \"customer_id\")\n",
        "    )\n",
        "\n",
        "    # 3. Shuffle Join (larger tables)\n",
        "    print(\"\\n3Ô∏è‚É£ Shuffle Join (Medium Tables):\")\n",
        "    time_shuffle = measure_join(\n",
        "        \"Shuffle join\",\n",
        "        lambda: df_large.join(df_medium, \"product_id\")\n",
        "    )\n",
        "\n",
        "    # 4. Bucketed Join Optimization\n",
        "    print(\"\\n4Ô∏è‚É£ Bucketed Join Setup:\")\n",
        "\n",
        "    # Save as bucketed tables (in production, you'd write to persistent storage)\n",
        "    print(\"   Creating bucketed tables...\")\n",
        "\n",
        "    # Bucket both tables by join key\n",
        "    df_large_bucketed = df_large.repartition(8, col(\"customer_id\"))\n",
        "    df_small_bucketed = df_small.repartition(8, col(\"customer_id\"))\n",
        "\n",
        "    time_bucketed = measure_join(\n",
        "        \"Pre-bucketed join\",\n",
        "        lambda: df_large_bucketed.join(df_small_bucketed, \"customer_id\")\n",
        "    )\n",
        "\n",
        "    # 5. Join with Data Skew Handling\n",
        "    print(\"\\n5Ô∏è‚É£ Handling Join Skew:\")\n",
        "\n",
        "    # Create skewed join scenario\n",
        "    skewed_large = df_large.filter(col(\"customer_id\") <= 10)  # Most data joins to few keys\n",
        "\n",
        "    # Traditional join (will be skewed)\n",
        "    time_skewed = measure_join(\n",
        "        \"Skewed join (traditional)\",\n",
        "        lambda: skewed_large.join(df_small, \"customer_id\")\n",
        "    )\n",
        "\n",
        "    # Skew handling with salting\n",
        "    df_salted = skewed_large.withColumn(\"salt\", (rand() * 3).cast(\"int\")) \\\n",
        "                           .withColumn(\"salted_key\", concat(col(\"customer_id\"), lit(\"_\"), col(\"salt\")))\n",
        "\n",
        "    df_small_expanded = df_small.withColumn(\"salt\", explode(array([lit(i) for i in range(3)]))) \\\n",
        "                              .withColumn(\"salted_key\", concat(col(\"customer_id\"), lit(\"_\"), col(\"salt\")))\n",
        "\n",
        "    time_salted = measure_join(\n",
        "        \"Skew handled with salting\",\n",
        "        lambda: df_salted.join(df_small_expanded, \"salted_key\")\n",
        "    )\n",
        "\n",
        "    # 6. Different Join Types Performance\n",
        "    print(\"\\n6Ô∏è‚É£ Join Type Comparison:\")\n",
        "\n",
        "    join_types = [\"inner\", \"left\", \"right\", \"outer\"]\n",
        "    join_times = {}\n",
        "\n",
        "    for join_type in join_types:\n",
        "        join_times[join_type] = measure_join(\n",
        "            f\"{join_type.capitalize()} join\",\n",
        "            lambda jt=join_type: df_large.join(df_small, \"customer_id\", jt)\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        'broadcast': time_broadcast,\n",
        "        'explicit_broadcast': time_explicit_broadcast,\n",
        "        'shuffle': time_shuffle,\n",
        "        'bucketed': time_bucketed,\n",
        "        'skewed': time_skewed,\n",
        "        'salted': time_salted,\n",
        "        'join_types': join_times\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# 3. ADVANCED CACHING STRATEGIES üíæ\n",
        "# =============================================================================\n",
        "\n",
        "def demo_advanced_caching():\n",
        "    \"\"\"Advanced caching strategies beyond basic persist\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üíæ ADVANCED CACHING STRATEGIES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create test dataset\n",
        "    data = [(i, f\"user_{i%1000}\", i * 10.5, i % 20, f\"2023-{(i%12)+1:02d}\")\n",
        "            for i in range(1, 200001)]\n",
        "    df = spark.createDataFrame(data, [\"id\", \"name\", \"amount\", \"category\", \"month\"])\n",
        "\n",
        "    print(f\"üìä Created dataset: {df.count():,} records\")\n",
        "\n",
        "    def test_storage_level(storage_level, name):\n",
        "        \"\"\"Test performance with different storage levels\"\"\"\n",
        "        df_cached = df.persist(storage_level)\n",
        "\n",
        "        start = time.time()\n",
        "        # First operation (caches data)\n",
        "        count1 = df_cached.count()\n",
        "        cache_time = time.time() - start\n",
        "\n",
        "        start = time.time()\n",
        "        # Second operation (uses cache)\n",
        "        count2 = df_cached.filter(col(\"amount\") > 1000).count()\n",
        "        use_time = time.time() - start\n",
        "\n",
        "        # Memory usage (simplified)\n",
        "        cached_info = \"Cached\" if df_cached.is_cached else \"Not cached\"\n",
        "\n",
        "        print(f\"   {name:20}: Cache={cache_time:.3f}s, Use={use_time:.3f}s, Status={cached_info}\")\n",
        "\n",
        "        df_cached.unpersist()\n",
        "        return cache_time, use_time\n",
        "\n",
        "    print(\"\\nüéØ Storage Level Comparison:\")\n",
        "\n",
        "    storage_results = {}\n",
        "\n",
        "    # Test different storage levels\n",
        "    levels = [\n",
        "        (StorageLevel.MEMORY_ONLY, \"MEMORY_ONLY\"),\n",
        "        (StorageLevel.MEMORY_AND_DISK, \"MEMORY_AND_DISK\"),\n",
        "        (StorageLevel.MEMORY_ONLY_SER, \"MEMORY_ONLY_SER\"),\n",
        "        (StorageLevel.MEMORY_AND_DISK_SER, \"MEMORY_AND_DISK_SER\"),\n",
        "        (StorageLevel.DISK_ONLY, \"DISK_ONLY\")\n",
        "    ]\n",
        "\n",
        "    for storage_level, name in levels:\n",
        "        cache_time, use_time = test_storage_level(storage_level, name)\n",
        "        storage_results[name] = {'cache': cache_time, 'use': use_time}\n",
        "\n",
        "    print(\"\\nüí° Storage Level Recommendations:\")\n",
        "    print(\"   ‚Ä¢ MEMORY_ONLY: Fast, but limited by memory size\")\n",
        "    print(\"   ‚Ä¢ MEMORY_AND_DISK: Balanced, most commonly used\")\n",
        "    print(\"   ‚Ä¢ *_SER: More memory efficient, slight CPU overhead\")\n",
        "    print(\"   ‚Ä¢ DISK_ONLY: Unlimited size, slowest access\")\n",
        "\n",
        "    # Cache Management Strategy\n",
        "    print(\"\\nüß† Cache Management Strategies:\")\n",
        "\n",
        "    # 1. Cache Reuse Pattern\n",
        "    print(\"\\n1Ô∏è‚É£ Cache Reuse Analysis:\")\n",
        "\n",
        "    df_reuse = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "    operations = [\n",
        "        (\"Count\", lambda: df_reuse.count()),\n",
        "        (\"Filter count\", lambda: df_reuse.filter(col(\"amount\") > 1000).count()),\n",
        "        (\"GroupBy\", lambda: df_reuse.groupBy(\"category\").sum(\"amount\").count()),\n",
        "        (\"Window function\", lambda: df_reuse.withColumn(\"rank\",\n",
        "                                   row_number().over(Window.partitionBy(\"category\").orderBy(\"amount\"))).count())\n",
        "    ]\n",
        "\n",
        "    for op_name, operation in operations:\n",
        "        start = time.time()\n",
        "        result = operation()\n",
        "        duration = time.time() - start\n",
        "        print(f\"   {op_name:15}: {duration:.3f}s\")\n",
        "\n",
        "    df_reuse.unpersist()\n",
        "\n",
        "    # 2. Memory Pressure Simulation\n",
        "    print(\"\\n2Ô∏è‚É£ Memory Pressure Handling:\")\n",
        "\n",
        "    # Cache multiple DataFrames to simulate memory pressure\n",
        "    dfs_cached = []\n",
        "    for i in range(3):\n",
        "        df_temp = df.sample(0.3).persist(StorageLevel.MEMORY_ONLY)\n",
        "        df_temp.count()  # Trigger caching\n",
        "        dfs_cached.append(df_temp)\n",
        "        print(f\"   Cached DF {i+1}: {df_temp.is_cached}\")\n",
        "\n",
        "    # Clean up\n",
        "    for df_temp in dfs_cached:\n",
        "        df_temp.unpersist()\n",
        "\n",
        "    # 3. Optimal Unpersist Timing\n",
        "    print(\"\\n3Ô∏è‚É£ Optimal Unpersist Strategy:\")\n",
        "\n",
        "    df_timed = df.persist()\n",
        "    df_timed.count()  # Cache\n",
        "\n",
        "    # Use cache multiple times\n",
        "    for i in range(3):\n",
        "        df_timed.filter(col(\"category\") == i).count()\n",
        "\n",
        "    print(\"   ‚úÖ Used cache for multiple operations\")\n",
        "\n",
        "    # Unpersist when no longer needed\n",
        "    df_timed.unpersist()\n",
        "    print(\"   üßπ Unpersisted to free memory\")\n",
        "\n",
        "    return storage_results\n",
        "\n",
        "# =============================================================================\n",
        "# 4. FILE FORMAT OPTIMIZATION üìÅ\n",
        "# =============================================================================\n",
        "\n",
        "def demo_file_format_optimization():\n",
        "    \"\"\"File format and storage optimization\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìÅ FILE FORMAT OPTIMIZATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create test data with different patterns\n",
        "    data = [(i, f\"user_{i%1000}\", i * 10.5, i % 20, f\"2023-{(i%12)+1:02d}\",\n",
        "             f\"country_{i%50}\", f\"product_{i%100}\")\n",
        "            for i in range(1, 100001)]\n",
        "\n",
        "    df = spark.createDataFrame(data, [\"id\", \"name\", \"amount\", \"category\", \"date\", \"country\", \"product\"])\n",
        "\n",
        "    print(f\"üìä Created dataset: {df.count():,} records\")\n",
        "\n",
        "    # File format comparison\n",
        "    print(\"\\nüéØ File Format Performance:\")\n",
        "\n",
        "    formats = [\"json\", \"csv\", \"parquet\"]\n",
        "    format_times = {}\n",
        "\n",
        "    for fmt in formats:\n",
        "        # Write\n",
        "        start = time.time()\n",
        "        df.write.mode(\"overwrite\").format(fmt).save(f\"/tmp/test_data_{fmt}\")\n",
        "        write_time = time.time() - start\n",
        "\n",
        "        # Read\n",
        "        start = time.time()\n",
        "        df_read = spark.read.format(fmt).load(f\"/tmp/test_data_{fmt}\")\n",
        "        count = df_read.count()\n",
        "        read_time = time.time() - start\n",
        "\n",
        "        format_times[fmt] = {'write': write_time, 'read': read_time}\n",
        "        print(f\"   {fmt.upper():8}: Write={write_time:.3f}s, Read={read_time:.3f}s\")\n",
        "\n",
        "    # Parquet optimization techniques\n",
        "    print(\"\\n‚ö° Parquet Optimization Techniques:\")\n",
        "\n",
        "    # 1. Column selection (projection pushdown)\n",
        "    print(\"\\n1Ô∏è‚É£ Column Projection:\")\n",
        "    start = time.time()\n",
        "    result1 = spark.read.parquet(\"/tmp/test_data_parquet\").select(\"id\", \"amount\").count()\n",
        "    time_projection = time.time() - start\n",
        "    print(f\"   Select 2 columns: {time_projection:.3f}s\")\n",
        "\n",
        "    start = time.time()\n",
        "    result2 = spark.read.parquet(\"/tmp/test_data_parquet\").count()\n",
        "    time_full = time.time() - start\n",
        "    print(f\"   Select all columns: {time_full:.3f}s\")\n",
        "\n",
        "    # 2. Predicate pushdown\n",
        "    print(\"\\n2Ô∏è‚É£ Predicate Pushdown:\")\n",
        "    start = time.time()\n",
        "    result3 = spark.read.parquet(\"/tmp/test_data_parquet\").filter(col(\"amount\") > 50000).count()\n",
        "    time_filter = time.time() - start\n",
        "    print(f\"   With filter: {time_filter:.3f}s\")\n",
        "\n",
        "    # 3. Partitioned datasets\n",
        "    print(\"\\n3Ô∏è‚É£ Dataset Partitioning:\")\n",
        "\n",
        "    # Write partitioned data\n",
        "    df.write.mode(\"overwrite\").partitionBy(\"category\").parquet(\"/tmp/partitioned_data\")\n",
        "\n",
        "    # Read with partition pruning\n",
        "    start = time.time()\n",
        "    result4 = spark.read.parquet(\"/tmp/partitioned_data\").filter(col(\"category\") == 5).count()\n",
        "    time_partitioned = time.time() - start\n",
        "    print(f\"   Partitioned read: {time_partitioned:.3f}s\")\n",
        "\n",
        "    return {\n",
        "        'formats': format_times,\n",
        "        'projection': time_projection,\n",
        "        'full': time_full,\n",
        "        'filter': time_filter,\n",
        "        'partitioned': time_partitioned\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# 5. MEMORY MANAGEMENT üß†\n",
        "# =============================================================================\n",
        "\n",
        "def demo_memory_management():\n",
        "    \"\"\"Memory management and configuration\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üß† MEMORY MANAGEMENT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Get current memory configuration\n",
        "    conf = spark.sparkContext.getConf()\n",
        "\n",
        "    print(\"üìä Current Memory Configuration:\")\n",
        "    memory_configs = [\n",
        "        \"spark.executor.memory\",\n",
        "        \"spark.executor.memoryFraction\",\n",
        "        \"spark.storage.memoryFraction\",\n",
        "        \"spark.sql.execution.arrow.maxRecordsPerBatch\"\n",
        "    ]\n",
        "\n",
        "    for config in memory_configs:\n",
        "        value = conf.get(config, \"Not set\")\n",
        "        print(f\"   {config:35}: {value}\")\n",
        "\n",
        "    # Memory usage analysis\n",
        "    print(\"\\nüîç Memory Usage Analysis:\")\n",
        "\n",
        "    # Create memory-intensive operations\n",
        "    large_data = [(i, f\"user_{i}\", \" \".join([f\"word_{j}\" for j in range(100)]))\n",
        "                  for i in range(1, 10001)]\n",
        "\n",
        "    df_memory = spark.createDataFrame(large_data, [\"id\", \"name\", \"description\"])\n",
        "\n",
        "    # Test different memory patterns\n",
        "    print(\"\\n1Ô∏è‚É£ Memory-intensive Operations:\")\n",
        "\n",
        "    # Cache and monitor\n",
        "    df_memory.persist(StorageLevel.MEMORY_ONLY)\n",
        "\n",
        "    start = time.time()\n",
        "    count = df_memory.count()\n",
        "    cache_time = time.time() - start\n",
        "    print(f\"   Cache operation: {cache_time:.3f}s\")\n",
        "\n",
        "    # Memory pressure test\n",
        "    start = time.time()\n",
        "    result = df_memory.rdd.map(lambda x: (x[0], len(x[2]))).collect()\n",
        "    collect_time = time.time() - start\n",
        "    print(f\"   Collect operation: {collect_time:.3f}s\")\n",
        "\n",
        "    df_memory.unpersist()\n",
        "\n",
        "    # Garbage collection impact\n",
        "    print(\"\\n2Ô∏è‚É£ GC Impact Analysis:\")\n",
        "\n",
        "    # Create objects that stress GC\n",
        "    for i in range(3):\n",
        "        temp_df = df_memory.sample(0.5)\n",
        "        temp_df.count()\n",
        "        print(f\"   Iteration {i+1}: Created temporary DataFrame\")\n",
        "\n",
        "    return {\n",
        "        'cache_time': cache_time,\n",
        "        'collect_time': collect_time\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# 6. CATALYST OPTIMIZER DEEP DIVE ‚ö°\n",
        "# =============================================================================\n",
        "\n",
        "def demo_catalyst_optimizer():\n",
        "    \"\"\"Understanding Catalyst optimizer behavior\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚ö° CATALYST OPTIMIZER DEEP DIVE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create test data\n",
        "    data = [(i, f\"user_{i%100}\", i * 10, i % 20, f\"2023-{(i%12)+1:02d}\")\n",
        "            for i in range(1, 50001)]\n",
        "    df = spark.createDataFrame(data, [\"id\", \"name\", \"amount\", \"category\", \"date\"])\n",
        "\n",
        "    print(f\"üìä Dataset: {df.count():,} records\")\n",
        "\n",
        "    # 1. Logical vs Physical Plans\n",
        "    print(\"\\n1Ô∏è‚É£ Query Plan Analysis:\")\n",
        "\n",
        "    query = df.filter(col(\"amount\") > 1000) \\\n",
        "              .select(\"name\", \"amount\", \"category\") \\\n",
        "              .groupBy(\"category\") \\\n",
        "              .avg(\"amount\")\n",
        "\n",
        "    print(\"\\nüìã Logical Plan:\")\n",
        "    query.explain(mode=\"simple\")\n",
        "\n",
        "    print(\"\\nüîß Physical Plan:\")\n",
        "    query.explain(mode=\"extended\")\n",
        "\n",
        "    # 2. Predicate Pushdown Demonstration\n",
        "    print(\"\\n2Ô∏è‚É£ Predicate Pushdown:\")\n",
        "\n",
        "    # Save as parquet to demonstrate pushdown\n",
        "    df.write.mode(\"overwrite\").parquet(\"/tmp/catalyst_demo\")\n",
        "\n",
        "    # Query with filter - should show pushdown in plan\n",
        "    filtered_query = spark.read.parquet(\"/tmp/catalyst_demo\") \\\n",
        "                          .filter(col(\"amount\") > 5000) \\\n",
        "                          .select(\"name\", \"amount\")\n",
        "\n",
        "    print(\"üîç Plan with predicate pushdown:\")\n",
        "    filtered_query.explain()\n",
        "\n",
        "    # 3. Column Pruning\n",
        "    print(\"\\n3Ô∏è‚É£ Column Pruning:\")\n",
        "\n",
        "    pruned_query = spark.read.parquet(\"/tmp/catalyst_demo\") \\\n",
        "                        .select(\"name\", \"amount\")  # Only these columns read\n",
        "\n",
        "    print(\"üîç Plan with column pruning:\")\n",
        "    pruned_query.explain()\n",
        "\n",
        "    # 4. Constant Folding\n",
        "    print(\"\\n4Ô∏è‚É£ Constant Folding:\")\n",
        "\n",
        "    # Catalyst will optimize constant expressions\n",
        "    constant_query = df.select(\n",
        "        col(\"name\"),\n",
        "        (lit(10) + lit(20)).alias(\"constant_sum\"),  # Will be folded to 30\n",
        "        (col(\"amount\") * 2).alias(\"doubled_amount\")\n",
        "    )\n",
        "\n",
        "    print(\"üîç Plan with constant folding:\")\n",
        "    constant_query.explain()\n",
        "\n",
        "    # 5. Join Optimization\n",
        "    print(\"\\n5Ô∏è‚É£ Join Optimization:\")\n",
        "\n",
        "    # Small table for broadcast join\n",
        "    small_data = [(i, f\"Category_{i}\") for i in range(1, 21)]\n",
        "    df_small = spark.createDataFrame(small_data, [\"category\", \"category_name\"])\n",
        "\n",
        "    # Catalyst should automatically choose broadcast join\n",
        "    join_query = df.join(df_small, \"category\")\n",
        "\n",
        "    print(\"üîç Automatic join optimization:\")\n",
        "    join_query.explain()\n",
        "\n",
        "    # 6. Cost-Based Optimization (CBO)\n",
        "    print(\"\\n6Ô∏è‚É£ Cost-Based Optimization:\")\n",
        "\n",
        "    # Enable CBO and generate statistics\n",
        "    spark.sql(\"SET spark.sql.cbo.enabled=true\")\n",
        "\n",
        "    # Create temporary view for SQL analysis\n",
        "    df.createOrReplaceTempView(\"sales_data\")\n",
        "    df_small.createOrReplaceTempView(\"categories\")\n",
        "\n",
        "    # CBO will use statistics to choose optimal plan\n",
        "    cbo_query = spark.sql(\"\"\"\n",
        "        SELECT c.category_name, AVG(s.amount) as avg_amount\n",
        "        FROM sales_data s\n",
        "        JOIN categories c ON s.category = c.category\n",
        "        WHERE s.amount > 2000\n",
        "        GROUP BY c.category_name\n",
        "        ORDER BY avg_amount DESC\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"üîç Cost-based optimized plan:\")\n",
        "    cbo_query.explain()\n",
        "\n",
        "    # 7. Adaptive Query Execution (AQE)\n",
        "    print(\"\\n7Ô∏è‚É£ Adaptive Query Execution:\")\n",
        "\n",
        "    print(\"üîß AQE Configuration:\")\n",
        "    aqe_configs = [\n",
        "        \"spark.sql.adaptive.enabled\",\n",
        "        \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
        "        \"spark.sql.adaptive.skewJoin.enabled\"\n",
        "    ]\n",
        "\n",
        "    for config in aqe_configs:\n",
        "        value = spark.conf.get(config, \"Not set\")\n",
        "        print(f\"   {config:40}: {value}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# =============================================================================\n",
        "# RUN ALL DEMOS\n",
        "# =============================================================================\n",
        "\n",
        "def run_all_demos():\n",
        "    \"\"\"Run all advanced PySpark demos\"\"\"\n",
        "    print(\"üé¨ Starting Advanced PySpark Tutorial\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Run each demo\n",
        "    results['partitioning'] = demo_partitioning_strategies()\n",
        "    results['joins'] = demo_join_optimizations()\n",
        "    results['caching'] = demo_advanced_caching()\n",
        "    results['formats'] = demo_file_format_optimization()\n",
        "    results['memory'] = demo_memory_management()\n",
        "    results['catalyst'] = demo_catalyst_optimizer()\n",
        "\n",
        "    # Summary visualization\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä PERFORMANCE SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create summary charts\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Partitioning performance\n",
        "    part_data = results['partitioning']\n",
        "    axes[0,0].bar(part_data.keys(), part_data.values())\n",
        "    axes[0,0].set_title('Partitioning Strategies Performance')\n",
        "    axes[0,0].set_ylabel('Time (seconds)')\n",
        "    axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Join performance\n",
        "    join_data = {k: v for k, v in results['joins'].items() if k != 'join_types'}\n",
        "    axes[0,1].bar(join_data.keys(), join_data.values())\n",
        "    axes[0,1].set_title('Join Optimization Performance')\n",
        "    axes[0,1].set_ylabel('Time (seconds)')\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Storage levels\n",
        "    cache_data = results['caching']\n",
        "    cache_times = [v['use'] for v in cache_data.values()]\n",
        "    cache_names = list(cache_data.keys())\n",
        "    axes[1,0].bar(cache_names, cache_times)\n",
        "    axes[1,0].set_title('Storage Level Performance')\n",
        "    axes[1,0].set_ylabel('Access Time (seconds)')\n",
        "    axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # File formats\n",
        "    format_data = results['formats']['formats']\n",
        "    format_names = list(format_data.keys())\n",
        "    read_times = [v['read'] for v in format_data.values()]\n",
        "    axes[1,1].bar(format_names, read_times)\n",
        "    axes[1,1].set_title('File Format Read Performance')\n",
        "    axes[1,1].set_ylabel('Read Time (seconds)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Key takeaways\n",
        "    print(\"\\nüéØ KEY TAKEAWAYS:\")\n",
        "    print(\"1. Choose partitioning strategy based on access patterns\")\n",
        "    print(\"2. Use broadcast joins for small tables\")\n",
        "    print(\"3. MEMORY_AND_DISK is usually the best storage level\")\n",
        "    print(\"4. Parquet with partitioning offers best performance\")\n",
        "    print(\"5. Monitor memory usage and GC impact\")\n",
        "    print(\"6. Catalyst optimizer works best with proper statistics\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# INTERACTIVE EXPLORATION GUIDE\n",
        "# =============================================================================\n",
        "\n",
        "def create_interactive_guide():\n",
        "    \"\"\"Create interactive exploration guide\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéÆ INTERACTIVE EXPLORATION GUIDE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\"\"\n",
        "üîç WHAT TO EXPLORE IN SPARK UI:\n",
        "\n",
        "1Ô∏è‚É£ PARTITIONING ANALYSIS:\n",
        "   ‚Ä¢ Jobs tab ‚Üí Look for shuffle operations\n",
        "   ‚Ä¢ Stages tab ‚Üí Check task distribution\n",
        "   ‚Ä¢ Look for: Uneven task execution times (skew)\n",
        "\n",
        "2Ô∏è‚É£ JOIN OPTIMIZATION:\n",
        "   ‚Ä¢ SQL tab ‚Üí Check join strategies in plans\n",
        "   ‚Ä¢ Look for: \"BroadcastHashJoin\" vs \"SortMergeJoin\"\n",
        "   ‚Ä¢ Stages tab ‚Üí Shuffle read/write volumes\n",
        "\n",
        "3Ô∏è‚É£ CACHE EFFECTIVENESS:\n",
        "   ‚Ä¢ Storage tab ‚Üí See cached DataFrames\n",
        "   ‚Ä¢ SQL tab ‚Üí \"InMemoryTableScan\" in plans\n",
        "   ‚Ä¢ Jobs tab ‚Üí Compare execution times\n",
        "\n",
        "4Ô∏è‚É£ FILE FORMAT IMPACT:\n",
        "   ‚Ä¢ SQL tab ‚Üí \"FileScan\" operations\n",
        "   ‚Ä¢ Look for: Predicate pushdown, column pruning\n",
        "   ‚Ä¢ Stages tab ‚Üí Input size differences\n",
        "\n",
        "5Ô∏è‚É£ MEMORY MANAGEMENT:\n",
        "   ‚Ä¢ Executors tab ‚Üí Memory usage patterns\n",
        "   ‚Ä¢ Look for: GC time, storage memory usage\n",
        "   ‚Ä¢ Storage tab ‚Üí Spill to disk indicators\n",
        "\n",
        "6Ô∏è‚É£ CATALYST OPTIMIZATIONS:\n",
        "   ‚Ä¢ SQL tab ‚Üí Physical vs logical plans\n",
        "   ‚Ä¢ Look for: Constant folding, predicate pushdown\n",
        "   ‚Ä¢ Check: Join reordering, projection pushdown\n",
        "    \"\"\")\n",
        "\n",
        "def create_troubleshooting_guide():\n",
        "    \"\"\"Create troubleshooting guide for common issues\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üîß TROUBLESHOOTING GUIDE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\"\"\n",
        "‚ùå COMMON ISSUES & SOLUTIONS:\n",
        "\n",
        "1Ô∏è‚É£ SLOW JOINS:\n",
        "   Problem: Large shuffle operations\n",
        "   Solutions:\n",
        "   ‚Ä¢ Use broadcast() for small tables (<200MB)\n",
        "   ‚Ä¢ Implement bucketing for repeated joins\n",
        "   ‚Ä¢ Use salting for skewed joins\n",
        "   ‚Ä¢ Check join keys for nulls\n",
        "\n",
        "2Ô∏è‚É£ OUT OF MEMORY ERRORS:\n",
        "   Problem: Executor memory exhaustion\n",
        "   Solutions:\n",
        "   ‚Ä¢ Increase executor memory\n",
        "   ‚Ä¢ Use MEMORY_AND_DISK storage level\n",
        "   ‚Ä¢ Reduce partition size with repartition()\n",
        "   ‚Ä¢ Avoid collect() on large datasets\n",
        "\n",
        "3Ô∏è‚É£ PARTITION SKEW:\n",
        "   Problem: Few tasks take much longer\n",
        "   Solutions:\n",
        "   ‚Ä¢ Add salt columns for better distribution\n",
        "   ‚Ä¢ Use repartitionByRange() for sorted data\n",
        "   ‚Ä¢ Check for hot keys in your data\n",
        "   ‚Ä¢ Consider custom partitioners\n",
        "\n",
        "4Ô∏è‚É£ CACHE NOT HELPING:\n",
        "   Problem: Cache overhead > benefit\n",
        "   Solutions:\n",
        "   ‚Ä¢ Only cache reused DataFrames\n",
        "   ‚Ä¢ Choose appropriate storage level\n",
        "   ‚Ä¢ Unpersist when no longer needed\n",
        "   ‚Ä¢ Consider serialized storage levels\n",
        "\n",
        "5Ô∏è‚É£ SLOW FILE READS:\n",
        "   Problem: Inefficient file formats\n",
        "   Solutions:\n",
        "   ‚Ä¢ Use Parquet instead of JSON/CSV\n",
        "   ‚Ä¢ Implement dataset partitioning\n",
        "   ‚Ä¢ Enable predicate pushdown\n",
        "   ‚Ä¢ Optimize file sizes (128MB-1GB)\n",
        "    \"\"\")\n",
        "\n",
        "def create_optimization_checklist():\n",
        "    \"\"\"Create optimization checklist\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ OPTIMIZATION CHECKLIST\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\"\"\n",
        "üöÄ PERFORMANCE OPTIMIZATION CHECKLIST:\n",
        "\n",
        "üìä DATA LAYOUT:\n",
        "   ‚òê Use Parquet format for analytical workloads\n",
        "   ‚òê Partition datasets by frequently filtered columns\n",
        "   ‚òê Optimize file sizes (avoid small files)\n",
        "   ‚òê Use appropriate compression (snappy/gzip)\n",
        "\n",
        "üîÑ PARTITIONING:\n",
        "   ‚òê Right-size partitions (128MB-1GB each)\n",
        "   ‚òê Use hash partitioning for joins\n",
        "   ‚òê Use range partitioning for ordered operations\n",
        "   ‚òê Avoid excessive repartitioning\n",
        "\n",
        "üîó JOINS:\n",
        "   ‚òê Broadcast small tables (<200MB)\n",
        "   ‚òê Use bucketing for repeated joins\n",
        "   ‚òê Handle data skew with salting\n",
        "   ‚òê Choose appropriate join types\n",
        "\n",
        "üíæ CACHING:\n",
        "   ‚òê Cache frequently accessed DataFrames\n",
        "   ‚òê Use MEMORY_AND_DISK for reliability\n",
        "   ‚òê Unpersist when no longer needed\n",
        "   ‚òê Monitor memory usage\n",
        "\n",
        "‚ö° CATALYST OPTIMIZATION:\n",
        "   ‚òê Enable adaptive query execution\n",
        "   ‚òê Use column pruning (select only needed columns)\n",
        "   ‚òê Push filters as early as possible\n",
        "   ‚òê Generate table statistics for CBO\n",
        "\n",
        "üß† MEMORY MANAGEMENT:\n",
        "   ‚òê Right-size executor memory\n",
        "   ‚òê Monitor GC overhead\n",
        "   ‚òê Use serialized storage when memory-constrained\n",
        "   ‚òê Avoid large broadcast variables\n",
        "\n",
        "üîç MONITORING:\n",
        "   ‚òê Regularly check Spark UI\n",
        "   ‚òê Monitor task execution times\n",
        "   ‚òê Watch for shuffle spill\n",
        "   ‚òê Track cache hit rates\n",
        "    \"\"\")\n",
        "\n",
        "def demo_real_world_scenarios():\n",
        "    \"\"\"Demonstrate real-world optimization scenarios\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üåç REAL-WORLD SCENARIOS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Scenario 1: ETL Pipeline\n",
        "    print(\"\\nüì¶ Scenario 1: ETL Pipeline Optimization\")\n",
        "\n",
        "    # Simulate daily sales data processing\n",
        "    daily_sales = [(i, f\"2023-{(i%12)+1:02d}-{(i%28)+1:02d}\",\n",
        "                   f\"product_{i%1000}\", i * 10.5, f\"store_{i%100}\")\n",
        "                  for i in range(1, 500001)]\n",
        "\n",
        "    df_sales = spark.createDataFrame(daily_sales,\n",
        "                                   [\"transaction_id\", \"date\", \"product_id\", \"amount\", \"store_id\"])\n",
        "\n",
        "    # ETL steps with optimization\n",
        "    print(\"üîß Optimized ETL Steps:\")\n",
        "\n",
        "    # Step 1: Partition by date for time-series queries\n",
        "    df_partitioned = df_sales.repartition(col(\"date\"))\n",
        "\n",
        "    # Step 2: Cache intermediate results\n",
        "    df_enriched = df_partitioned.withColumn(\"year_month\", substring(\"date\", 1, 7)) \\\n",
        "                               .persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "    # Step 3: Multiple aggregations using cache\n",
        "    start = time.time()\n",
        "\n",
        "    monthly_sales = df_enriched.groupBy(\"year_month\").sum(\"amount\")\n",
        "    store_sales = df_enriched.groupBy(\"store_id\").sum(\"amount\")\n",
        "    product_sales = df_enriched.groupBy(\"product_id\").sum(\"amount\")\n",
        "\n",
        "    # Trigger computations\n",
        "    monthly_count = monthly_sales.count()\n",
        "    store_count = store_sales.count()\n",
        "    product_count = product_sales.count()\n",
        "\n",
        "    etl_time = time.time() - start\n",
        "    print(f\"   ‚úÖ ETL completed in {etl_time:.3f}s\")\n",
        "    print(f\"   üìä Generated {monthly_count} monthly, {store_count} store, {product_count} product aggregates\")\n",
        "\n",
        "    df_enriched.unpersist()\n",
        "\n",
        "    # Scenario 2: Machine Learning Feature Engineering\n",
        "    print(\"\\nü§ñ Scenario 2: ML Feature Engineering\")\n",
        "\n",
        "    # Simulate user behavior data\n",
        "    user_data = [(i, f\"user_{i%10000}\", i % 100, (i * 7) % 24, i % 7)\n",
        "                for i in range(1, 1000001)]\n",
        "\n",
        "    df_users = spark.createDataFrame(user_data,\n",
        "                                   [\"session_id\", \"user_id\", \"page_views\", \"hour\", \"day_of_week\"])\n",
        "\n",
        "    # Feature engineering with window functions\n",
        "    start = time.time()\n",
        "\n",
        "    # Cache raw data for multiple feature computations\n",
        "    df_users.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "    # Create features using window functions\n",
        "    user_window = Window.partitionBy(\"user_id\").orderBy(\"session_id\")\n",
        "\n",
        "    df_features = df_users.withColumn(\"prev_page_views\",\n",
        "                                    lag(\"page_views\").over(user_window)) \\\n",
        "                         .withColumn(\"session_rank\",\n",
        "                                   row_number().over(user_window)) \\\n",
        "                         .withColumn(\"avg_page_views\",\n",
        "                                   avg(\"page_views\").over(user_window.rowsBetween(-2, 0)))\n",
        "\n",
        "    feature_count = df_features.count()\n",
        "    feature_time = time.time() - start\n",
        "\n",
        "    print(f\"   ‚úÖ Feature engineering completed in {feature_time:.3f}s\")\n",
        "    print(f\"   üéØ Generated features for {feature_count:,} sessions\")\n",
        "\n",
        "    df_users.unpersist()\n",
        "\n",
        "    # Scenario 3: Real-time Analytics Dashboard\n",
        "    print(\"\\nüìà Scenario 3: Real-time Analytics\")\n",
        "\n",
        "    # Simulate streaming-like batch processing\n",
        "    batch_data = [(i, f\"event_{i%50}\", int(time.time()) + i, i % 10)\n",
        "                 for i in range(1, 100001)]\n",
        "\n",
        "    df_events = spark.createDataFrame(batch_data,\n",
        "                                    [\"event_id\", \"event_type\", \"timestamp\", \"user_segment\"])\n",
        "\n",
        "    # Real-time aggregations\n",
        "    start = time.time()\n",
        "\n",
        "    # Multiple real-time metrics\n",
        "    hourly_events = df_events.withColumn(\"hour\",\n",
        "                                       from_unixtime(\"timestamp\", \"yyyy-MM-dd HH\")) \\\n",
        "                           .groupBy(\"hour\", \"event_type\") \\\n",
        "                           .count()\n",
        "\n",
        "    segment_metrics = df_events.groupBy(\"user_segment\") \\\n",
        "                             .agg(count(\"*\").alias(\"event_count\"),\n",
        "                                 countDistinct(\"event_type\").alias(\"unique_events\"))\n",
        "\n",
        "    # Execute in parallel\n",
        "    hourly_count = hourly_events.count()\n",
        "    segment_count = segment_metrics.count()\n",
        "\n",
        "    dashboard_time = time.time() - start\n",
        "\n",
        "    print(f\"   ‚úÖ Dashboard metrics computed in {dashboard_time:.3f}s\")\n",
        "    print(f\"   üìä {hourly_count} hourly metrics, {segment_count} segment metrics\")\n",
        "\n",
        "    return {\n",
        "        'etl_time': etl_time,\n",
        "        'feature_time': feature_time,\n",
        "        'dashboard_time': dashboard_time\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup ngrok for Spark UI\n",
        "    try:\n",
        "        from pyngrok import ngrok\n",
        "        ngrok.set_auth_token(\"2pPrNsyDH2wB1rSvXmQH4fFYTpk_4n14U3YrsvxTVLLDx5D4v\")\n",
        "        public_url = ngrok.connect(4040)\n",
        "        print(f\"üåê Spark UI available at: {public_url}\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è  Ngrok not available, use localhost:4040 for Spark UI\")\n",
        "\n",
        "    # Run all demos\n",
        "    demo_results = run_all_demos()\n",
        "\n",
        "    # Create guides\n",
        "    create_interactive_guide()\n",
        "    create_troubleshooting_guide()\n",
        "    create_optimization_checklist()\n",
        "\n",
        "    # Real-world scenarios\n",
        "    scenario_results = demo_real_world_scenarios()\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üéì ADVANCED PYSPARK TUTORIAL COMPLETED!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\"\"\n",
        "üìö TOPICS COVERED:\n",
        "‚úÖ Partitioning Strategies (repartition vs coalesce, custom partitioning)\n",
        "‚úÖ Join Optimizations (broadcast, bucketing, skew handling)\n",
        "‚úÖ Advanced Caching (storage levels, memory management)\n",
        "‚úÖ File Format Optimization (Parquet, partitioning, compression)\n",
        "‚úÖ Memory Management (GC tuning, memory fractions)\n",
        "‚úÖ Catalyst Optimizer (predicate pushdown, CBO, AQE)\n",
        "\n",
        "üéØ NEXT STEPS:\n",
        "1. Practice these techniques with your own datasets\n",
        "2. Monitor performance using Spark UI\n",
        "3. Apply optimization patterns to production workloads\n",
        "4. Explore streaming and MLlib for advanced use cases\n",
        "\n",
        "üîó Keep the Spark UI open to explore execution plans and performance metrics!\n",
        "    \"\"\")\n",
        "\n",
        "    # Keep session alive for UI exploration\n",
        "    print(\"\\nüí° Session kept alive for UI exploration...\")\n",
        "    print(\"   Run spark.stop() when finished\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iIRZx6MptZTv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}