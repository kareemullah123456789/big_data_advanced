{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29dfe78",
   "metadata": {},
   "source": [
    "# ðŸš€ PySpark Complete Tutorial: Basics â†’ Intermediate â†’ Advanced\n",
    "## For Databricks Serverless Free Account\n",
    "\n",
    "This notebook provides a comprehensive guide to PySpark on Databricks Serverless Compute. All examples use inline sample data â€” no external files required.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Databricks Community Edition (free) or Databricks Serverless account\n",
    "- `spark` session is pre-configured in Databricks notebooks\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "**PART 1 â€” BASICS**\n",
    "1. Install & Import Libraries\n",
    "2. Initialize Spark Session\n",
    "3. Creating DataFrames\n",
    "4. Basic DataFrame Operations\n",
    "5. Column Manipulations & Type Casting\n",
    "6. Aggregations & GroupBy\n",
    "7. Working with Null Values & Data Cleaning\n",
    "\n",
    "**PART 2 â€” INTERMEDIATE**\n",
    "8. Joins\n",
    "9. Window Functions\n",
    "10. User Defined Functions (UDFs) & Pandas UDFs\n",
    "11. Complex Data Types: Arrays, Maps, Structs\n",
    "12. Spark SQL: Temp Views & SQL Queries\n",
    "\n",
    "**PART 3 â€” ADVANCED**\n",
    "13. Reading & Writing Data in Delta Lake\n",
    "14. Caching, Persistence & Performance Tuning\n",
    "15. Higher-Order Functions\n",
    "16. Broadcast Variables & Accumulators\n",
    "17. Partitioning, Bucketing & AQE\n",
    "18. Structured Streaming Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a074892",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1 â€” BASICS\n",
    "---\n",
    "## 1. Install & Import PySpark Libraries\n",
    "\n",
    "In Databricks, PySpark is pre-installed. On local environments, install with `pip install pyspark`.\n",
    "Below we import the essential modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45adeec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Databricks, these are pre-installed. For local, run: pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType,\n",
    "    FloatType, DoubleType, DateType, TimestampType,\n",
    "    BooleanType, ArrayType, MapType, LongType\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581819a",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session for Databricks Serverless\n",
    "\n",
    "In **Databricks**, a `spark` session is automatically available â€” you don't need to create one.  \n",
    "Databricks Serverless Compute manages cluster resources automatically (no cluster configuration needed).\n",
    "\n",
    "The code below is for **local/non-Databricks** environments. In Databricks, you can skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841db892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Databricks, `spark` is already available. This is for local environments only.\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"PySpark Tutorial\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Verify the session\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcf238",
   "metadata": {},
   "source": [
    "## 3. Creating DataFrames from Various Sources\n",
    "\n",
    "DataFrames are the primary data structure in PySpark. You can create them from:\n",
    "- Python lists of tuples/rows\n",
    "- Python dictionaries (via Pandas)\n",
    "- Explicit schemas with `StructType`\n",
    "- External files (CSV, JSON, Parquet, Delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2967d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Method 1: From a list of tuples with column names ---\n",
    "data = [\n",
    "    (\"Alice\", \"Engineering\", 95000, 30, \"2020-01-15\"),\n",
    "    (\"Bob\", \"Marketing\", 72000, 28, \"2021-03-22\"),\n",
    "    (\"Charlie\", \"Engineering\", 110000, 35, \"2018-07-10\"),\n",
    "    (\"Diana\", \"HR\", 68000, 26, \"2022-06-01\"),\n",
    "    (\"Eve\", \"Marketing\", 85000, 32, \"2019-11-30\"),\n",
    "    (\"Frank\", \"Engineering\", 102000, 40, \"2017-04-18\"),\n",
    "    (\"Grace\", \"HR\", 71000, 29, \"2021-09-05\"),\n",
    "    (\"Hank\", \"Sales\", 78000, 34, \"2020-02-14\"),\n",
    "    (\"Ivy\", \"Sales\", 92000, 38, \"2018-12-20\"),\n",
    "    (\"Jack\", \"Engineering\", 115000, 45, \"2016-08-25\"),\n",
    "]\n",
    "columns = [\"name\", \"department\", \"salary\", \"age\", \"hire_date\"]\n",
    "\n",
    "df_employees = spark.createDataFrame(data, columns)\n",
    "df_employees.show()\n",
    "print(f\"Row count: {df_employees.count()}\")\n",
    "print(f\"Columns: {df_employees.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204704d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Method 2: With an explicit schema using StructType ---\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True),\n",
    "])\n",
    "\n",
    "df_typed = spark.createDataFrame(data, schema=schema)\n",
    "df_typed.printSchema()\n",
    "df_typed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Method 3: From a Pandas DataFrame ---\n",
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame({\n",
    "    \"product\": [\"Laptop\", \"Phone\", \"Tablet\", \"Monitor\", \"Keyboard\"],\n",
    "    \"price\": [999.99, 699.99, 449.99, 329.99, 79.99],\n",
    "    \"quantity\": [50, 200, 150, 80, 500]\n",
    "})\n",
    "\n",
    "df_products = spark.createDataFrame(pandas_df)\n",
    "df_products.show()\n",
    "df_products.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68685e",
   "metadata": {},
   "source": [
    "## 4. Basic DataFrame Operations: Select, Filter, and Sort\n",
    "\n",
    "Core operations for exploring and transforming DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e227944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SELECT: Different ways to select columns ---\n",
    "# Method 1: Column names as strings\n",
    "df_employees.select(\"name\", \"salary\").show(5)\n",
    "\n",
    "# Method 2: Using col()\n",
    "df_employees.select(F.col(\"name\"), F.col(\"salary\")).show(5)\n",
    "\n",
    "# Method 3: Using DataFrame column reference\n",
    "df_employees.select(df_employees.name, df_employees[\"salary\"]).show(5)\n",
    "\n",
    "# --- describe() for summary statistics ---\n",
    "df_employees.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FILTER / WHERE: Filter rows based on conditions ---\n",
    "# Employees with salary > 90000\n",
    "df_employees.filter(F.col(\"salary\") > 90000).show()\n",
    "\n",
    "# Equivalent using where()\n",
    "df_employees.where(\"salary > 90000\").show()\n",
    "\n",
    "# Multiple conditions with AND (&) and OR (|)\n",
    "df_employees.filter(\n",
    "    (F.col(\"department\") == \"Engineering\") & (F.col(\"age\") > 30)\n",
    ").show()\n",
    "\n",
    "# Using isin() for multiple values\n",
    "df_employees.filter(F.col(\"department\").isin(\"Engineering\", \"Sales\")).show()\n",
    "\n",
    "# Using like() for pattern matching\n",
    "df_employees.filter(F.col(\"name\").like(\"A%\")).show()\n",
    "\n",
    "# Using between()\n",
    "df_employees.filter(F.col(\"salary\").between(70000, 100000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7882035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SORT / ORDER BY ---\n",
    "# Sort by salary descending\n",
    "df_employees.orderBy(F.col(\"salary\").desc()).show()\n",
    "\n",
    "# Sort by multiple columns\n",
    "df_employees.orderBy(\"department\", F.col(\"salary\").desc()).show()\n",
    "\n",
    "# --- DISTINCT and LIMIT ---\n",
    "df_employees.select(\"department\").distinct().show()\n",
    "df_employees.limit(3).show()\n",
    "\n",
    "# --- dtypes: Quick view of column types ---\n",
    "print(\"Column types:\", df_employees.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a26982",
   "metadata": {},
   "source": [
    "## 5. Column Manipulations & Type Casting\n",
    "\n",
    "Add, rename, transform, and cast columns. Includes common string and date functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d723bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- withColumn(): Add or transform columns ---\n",
    "df_transformed = (\n",
    "    df_employees\n",
    "    # Add a new column: annual bonus (10% of salary)\n",
    "    .withColumn(\"bonus\", F.col(\"salary\") * 0.10)\n",
    "    # Add a constant column\n",
    "    .withColumn(\"country\", F.lit(\"USA\"))\n",
    "    # Cast hire_date string to DateType\n",
    "    .withColumn(\"hire_date\", F.to_date(F.col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    "    # Calculate years of experience from hire_date\n",
    "    .withColumn(\"years_exp\", F.round(F.datediff(F.current_date(), F.col(\"hire_date\")) / 365, 1))\n",
    ")\n",
    "\n",
    "df_transformed.show()\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rename and Drop columns ---\n",
    "df_renamed = df_employees.withColumnRenamed(\"name\", \"employee_name\")\n",
    "df_renamed.show(3)\n",
    "\n",
    "df_dropped = df_employees.drop(\"age\", \"hire_date\")\n",
    "df_dropped.show(3)\n",
    "\n",
    "# --- String Functions ---\n",
    "df_string = df_employees.select(\n",
    "    F.col(\"name\"),\n",
    "    F.upper(F.col(\"name\")).alias(\"name_upper\"),\n",
    "    F.lower(F.col(\"name\")).alias(\"name_lower\"),\n",
    "    F.length(F.col(\"name\")).alias(\"name_length\"),\n",
    "    F.substring(F.col(\"name\"), 1, 3).alias(\"first_3_chars\"),\n",
    "    F.concat(F.col(\"name\"), F.lit(\" - \"), F.col(\"department\")).alias(\"name_dept\"),\n",
    "    F.regexp_replace(F.col(\"department\"), \"Engineering\", \"Eng\").alias(\"dept_short\"),\n",
    "    F.trim(F.col(\"name\")).alias(\"name_trimmed\"),\n",
    ")\n",
    "df_string.show(truncate=False)\n",
    "\n",
    "# --- Type Casting ---\n",
    "df_cast = df_employees.select(\n",
    "    F.col(\"salary\").cast(\"double\").alias(\"salary_double\"),\n",
    "    F.col(\"salary\").cast(StringType()).alias(\"salary_string\"),\n",
    "    F.col(\"age\").cast(\"string\").alias(\"age_string\"),\n",
    ")\n",
    "df_cast.show(5)\n",
    "df_cast.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c26881",
   "metadata": {},
   "source": [
    "## 6. Aggregations & GroupBy Operations\n",
    "\n",
    "Perform grouped computations using `groupBy()` with aggregate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b410d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic GroupBy with single aggregate ---\n",
    "df_employees.groupBy(\"department\").count().show()\n",
    "\n",
    "# --- GroupBy with multiple aggregations using agg() ---\n",
    "df_agg = df_employees.groupBy(\"department\").agg(\n",
    "    F.count(\"*\").alias(\"emp_count\"),\n",
    "    F.sum(\"salary\").alias(\"total_salary\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.round(F.stddev(\"salary\"), 2).alias(\"stddev_salary\"),\n",
    ")\n",
    "df_agg.orderBy(\"department\").show()\n",
    "\n",
    "# --- Pivot Table: Department salary by age group ---\n",
    "df_pivot = (\n",
    "    df_employees\n",
    "    .withColumn(\"age_group\",\n",
    "        F.when(F.col(\"age\") < 30, \"Under 30\")\n",
    "         .when(F.col(\"age\") < 40, \"30-39\")\n",
    "         .otherwise(\"40+\")\n",
    "    )\n",
    "    .groupBy(\"department\")\n",
    "    .pivot(\"age_group\")\n",
    "    .agg(F.round(F.avg(\"salary\"), 0))\n",
    ")\n",
    "df_pivot.show()\n",
    "\n",
    "# --- Global Aggregations (without groupBy) ---\n",
    "df_employees.agg(\n",
    "    F.sum(\"salary\").alias(\"total_payroll\"),\n",
    "    F.avg(\"age\").alias(\"avg_age\"),\n",
    "    F.countDistinct(\"department\").alias(\"num_departments\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c7c681",
   "metadata": {},
   "source": [
    "## 7. Working with Null Values & Data Cleaning\n",
    "\n",
    "Handle missing data, duplicates, and data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da01c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with null values for demonstration\n",
    "data_nulls = [\n",
    "    (\"Alice\", \"Engineering\", 95000, \"alice@co.com\"),\n",
    "    (\"Bob\", None, 72000, \"bob@co.com\"),\n",
    "    (\"Charlie\", \"Engineering\", None, None),\n",
    "    (\"Diana\", \"HR\", 68000, \"diana@co.com\"),\n",
    "    (\"Eve\", None, None, \"eve@co.com\"),\n",
    "    (\"Alice\", \"Engineering\", 95000, \"alice@co.com\"),  # duplicate\n",
    "]\n",
    "df_dirty = spark.createDataFrame(data_nulls, [\"name\", \"department\", \"salary\", \"email\"])\n",
    "print(\"=== Original (with nulls & duplicates) ===\")\n",
    "df_dirty.show()\n",
    "\n",
    "# --- Check for nulls ---\n",
    "df_dirty.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df_dirty.columns]).show()\n",
    "\n",
    "# --- Drop rows with ANY null ---\n",
    "print(\"=== Drop rows with any null ===\")\n",
    "df_dirty.na.drop(\"any\").show()\n",
    "\n",
    "# --- Drop rows where specific columns are null ---\n",
    "print(\"=== Drop rows where department is null ===\")\n",
    "df_dirty.na.drop(subset=[\"department\"]).show()\n",
    "\n",
    "# --- Fill nulls with defaults ---\n",
    "print(\"=== Fill nulls ===\")\n",
    "df_dirty.na.fill({\"department\": \"Unknown\", \"salary\": 0, \"email\": \"N/A\"}).show()\n",
    "\n",
    "# --- Using coalesce() to pick first non-null value ---\n",
    "df_dirty.select(\n",
    "    \"name\",\n",
    "    F.coalesce(F.col(\"department\"), F.lit(\"Unassigned\")).alias(\"department\")\n",
    ").show()\n",
    "\n",
    "# --- Filter nulls ---\n",
    "df_dirty.filter(F.col(\"department\").isNull()).show()\n",
    "df_dirty.filter(F.col(\"department\").isNotNull()).show()\n",
    "\n",
    "# --- Remove duplicates ---\n",
    "print(\"=== After dropDuplicates ===\")\n",
    "df_dirty.dropDuplicates().show()\n",
    "df_dirty.dropDuplicates([\"name\"]).show()  # based on specific columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228e6d3",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2 â€” INTERMEDIATE\n",
    "---\n",
    "## 8. Joins: Inner, Outer, Left, Right, and Cross\n",
    "\n",
    "Combine DataFrames using various join types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create two DataFrames for join examples ---\n",
    "employees_data = [\n",
    "    (1, \"Alice\", 101), (2, \"Bob\", 102), (3, \"Charlie\", 101),\n",
    "    (4, \"Diana\", 103), (5, \"Eve\", None),\n",
    "]\n",
    "departments_data = [\n",
    "    (101, \"Engineering\", \"Building A\"),\n",
    "    (102, \"Marketing\", \"Building B\"),\n",
    "    (103, \"HR\", \"Building C\"),\n",
    "    (104, \"Finance\", \"Building D\"),\n",
    "]\n",
    "\n",
    "df_emp = spark.createDataFrame(employees_data, [\"emp_id\", \"name\", \"dept_id\"])\n",
    "df_dept = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\", \"location\"])\n",
    "\n",
    "print(\"=== Employees ===\")\n",
    "df_emp.show()\n",
    "print(\"=== Departments ===\")\n",
    "df_dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7666a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INNER JOIN: Only matching rows ---\n",
    "print(\"=== INNER JOIN ===\")\n",
    "df_emp.join(df_dept, df_emp.dept_id == df_dept.dept_id, \"inner\") \\\n",
    "    .drop(df_dept.dept_id).show()\n",
    "\n",
    "# --- LEFT JOIN: All from left + matching from right ---\n",
    "print(\"=== LEFT JOIN ===\")\n",
    "df_emp.join(df_dept, df_emp.dept_id == df_dept.dept_id, \"left\") \\\n",
    "    .drop(df_dept.dept_id).show()\n",
    "\n",
    "# --- RIGHT JOIN: All from right + matching from left ---\n",
    "print(\"=== RIGHT JOIN ===\")\n",
    "df_emp.join(df_dept, df_emp.dept_id == df_dept.dept_id, \"right\") \\\n",
    "    .drop(df_emp.dept_id).show()\n",
    "\n",
    "# --- FULL OUTER JOIN: All rows from both ---\n",
    "print(\"=== FULL OUTER JOIN ===\")\n",
    "df_emp.join(df_dept, df_emp.dept_id == df_dept.dept_id, \"full\") \\\n",
    "    .show()\n",
    "\n",
    "# --- LEFT SEMI JOIN: Rows from left that have a match (like EXISTS) ---\n",
    "print(\"=== LEFT SEMI JOIN ===\")\n",
    "df_emp.join(df_dept, df_emp.dept_id == df_dept.dept_id, \"left_semi\").show()\n",
    "\n",
    "# --- LEFT ANTI JOIN: Rows from left that do NOT match (like NOT EXISTS) ---\n",
    "print(\"=== LEFT ANTI JOIN ===\")\n",
    "df_emp.join(df_dept, df_emp.dept_id == df_dept.dept_id, \"left_anti\").show()\n",
    "\n",
    "# --- CROSS JOIN: Cartesian product ---\n",
    "print(f\"=== CROSS JOIN ({df_emp.count()} x {df_dept.count()} = {df_emp.count() * df_dept.count()} rows) ===\")\n",
    "df_emp.crossJoin(df_dept).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827aa503",
   "metadata": {},
   "source": [
    "## 9. Window Functions: Rank, Row Number, and Running Totals\n",
    "\n",
    "Window functions perform calculations across a set of rows related to the current row â€” without collapsing them like `groupBy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b792891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Window Specifications ---\n",
    "window_dept = Window.partitionBy(\"department\").orderBy(F.col(\"salary\").desc())\n",
    "window_dept_rows = Window.partitionBy(\"department\").orderBy(\"salary\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# --- Ranking Functions ---\n",
    "df_window = df_employees.select(\n",
    "    \"name\", \"department\", \"salary\",\n",
    "    F.row_number().over(window_dept).alias(\"row_num\"),\n",
    "    F.rank().over(window_dept).alias(\"rank\"),\n",
    "    F.dense_rank().over(window_dept).alias(\"dense_rank\"),\n",
    "    F.ntile(2).over(window_dept).alias(\"ntile_2\"),\n",
    ")\n",
    "df_window.show()\n",
    "\n",
    "# --- Lag and Lead: Access previous/next row ---\n",
    "window_by_salary = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df_lag_lead = df_employees.select(\n",
    "    \"name\", \"department\", \"salary\",\n",
    "    F.lag(\"salary\", 1).over(window_by_salary).alias(\"prev_salary\"),\n",
    "    F.lead(\"salary\", 1).over(window_by_salary).alias(\"next_salary\"),\n",
    "    (F.col(\"salary\") - F.lag(\"salary\", 1).over(window_by_salary)).alias(\"salary_diff\"),\n",
    ")\n",
    "df_lag_lead.show()\n",
    "\n",
    "# --- Running Totals and Moving Averages ---\n",
    "df_running = df_employees.select(\n",
    "    \"name\", \"department\", \"salary\",\n",
    "    F.sum(\"salary\").over(window_dept_rows).alias(\"running_total\"),\n",
    "    F.avg(\"salary\").over(window_dept_rows).alias(\"running_avg\"),\n",
    "    F.count(\"*\").over(Window.partitionBy(\"department\")).alias(\"dept_total_count\"),\n",
    ")\n",
    "df_running.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4429832f",
   "metadata": {},
   "source": [
    "## 10. User Defined Functions (UDFs) & Pandas UDFs\n",
    "\n",
    "Create custom functions when built-in functions aren't enough. **Pandas UDFs** (vectorized) are significantly faster than regular UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23721b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, pandas_udf\n",
    "\n",
    "# === Method 1: Regular UDF with decorator ===\n",
    "@udf(returnType=StringType())\n",
    "def salary_band(salary):\n",
    "    if salary is None:\n",
    "        return \"Unknown\"\n",
    "    elif salary < 75000:\n",
    "        return \"Junior\"\n",
    "    elif salary < 100000:\n",
    "        return \"Mid\"\n",
    "    else:\n",
    "        return \"Senior\"\n",
    "\n",
    "df_employees.select(\"name\", \"salary\", salary_band(\"salary\").alias(\"band\")).show()\n",
    "\n",
    "# === Method 2: Register UDF for use in SQL ===\n",
    "spark.udf.register(\"salary_band_sql\", lambda s: \"Junior\" if s and s < 75000 else (\"Mid\" if s and s < 100000 else \"Senior\"), StringType())\n",
    "\n",
    "# === Method 3: Pandas UDF (Vectorized â€” MUCH faster) ===\n",
    "@pandas_udf(DoubleType())\n",
    "def tax_amount(salary: pd.Series) -> pd.Series:\n",
    "    \"\"\"Calculate tax: 20% for salary > 90K, else 15%\"\"\"\n",
    "    return salary.apply(lambda s: s * 0.20 if s > 90000 else s * 0.15)\n",
    "\n",
    "df_employees.select(\n",
    "    \"name\", \"salary\",\n",
    "    tax_amount(F.col(\"salary\")).alias(\"tax\"),\n",
    "    (F.col(\"salary\") - tax_amount(F.col(\"salary\"))).alias(\"net_salary\"),\n",
    ").show()\n",
    "\n",
    "# === Performance note ===\n",
    "# Regular UDFs: row-by-row processing (slow, serialization overhead)\n",
    "# Pandas UDFs: batch processing with Arrow (fast, vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bac04e",
   "metadata": {},
   "source": [
    "## 11. Working with Complex Data Types: Arrays, Maps, and Structs\n",
    "\n",
    "PySpark supports nested and complex types for semi-structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Arrays ---\n",
    "df_skills = spark.createDataFrame([\n",
    "    (\"Alice\", [\"Python\", \"Spark\", \"SQL\"]),\n",
    "    (\"Bob\", [\"Java\", \"Scala\"]),\n",
    "    (\"Charlie\", [\"Python\", \"R\", \"Spark\", \"ML\"]),\n",
    "], [\"name\", \"skills\"])\n",
    "\n",
    "df_skills.show(truncate=False)\n",
    "df_skills.printSchema()\n",
    "\n",
    "# explode: one row per array element\n",
    "df_skills.select(\"name\", F.explode(\"skills\").alias(\"skill\")).show()\n",
    "\n",
    "# posexplode: with position index\n",
    "df_skills.select(\"name\", F.posexplode(\"skills\").alias(\"pos\", \"skill\")).show()\n",
    "\n",
    "# Array functions\n",
    "df_skills.select(\n",
    "    \"name\",\n",
    "    F.size(\"skills\").alias(\"num_skills\"),\n",
    "    F.array_contains(\"skills\", \"Python\").alias(\"knows_python\"),\n",
    "    F.array_sort(\"skills\").alias(\"skills_sorted\"),\n",
    "    F.array_distinct(\"skills\").alias(\"skills_unique\"),\n",
    ").show(truncate=False)\n",
    "\n",
    "# collect_list and collect_set (reverse of explode)\n",
    "df_exploded = df_skills.select(\"name\", F.explode(\"skills\").alias(\"skill\"))\n",
    "df_exploded.groupBy(\"name\").agg(\n",
    "    F.collect_list(\"skill\").alias(\"skills_list\"),\n",
    "    F.collect_set(\"skill\").alias(\"skills_set\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Maps (Key-Value pairs) ---\n",
    "df_maps = spark.createDataFrame([\n",
    "    (\"Alice\", {\"Python\": 5, \"SQL\": 4, \"Spark\": 3}),\n",
    "    (\"Bob\", {\"Java\": 4, \"Scala\": 3}),\n",
    "], [\"name\", \"skill_ratings\"])\n",
    "\n",
    "df_maps.show(truncate=False)\n",
    "\n",
    "df_maps.select(\n",
    "    \"name\",\n",
    "    F.map_keys(\"skill_ratings\").alias(\"skills\"),\n",
    "    F.map_values(\"skill_ratings\").alias(\"ratings\"),\n",
    "    F.col(\"skill_ratings\")[\"Python\"].alias(\"python_rating\"),\n",
    ").show(truncate=False)\n",
    "\n",
    "# Explode map into key-value rows\n",
    "df_maps.select(\"name\", F.explode(\"skill_ratings\").alias(\"skill\", \"rating\")).show()\n",
    "\n",
    "# --- Structs (Nested objects) ---\n",
    "df_struct = spark.createDataFrame([\n",
    "    (\"Alice\", (\"123 Main St\", \"NYC\", \"NY\")),\n",
    "    (\"Bob\", (\"456 Oak Ave\", \"LA\", \"CA\")),\n",
    "], [\"name\", \"address\"])\n",
    "\n",
    "# Access struct fields\n",
    "df_struct.select(\"name\", \"address.*\").show()\n",
    "df_struct.select(\"name\", F.col(\"address._1\").alias(\"street\")).show()\n",
    "\n",
    "# Create struct from columns\n",
    "df_employees.select(\n",
    "    \"name\",\n",
    "    F.struct(\"department\", \"salary\").alias(\"job_info\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38556440",
   "metadata": {},
   "source": [
    "## 12. Spark SQL: Temporary Views and SQL Queries\n",
    "\n",
    "Mix DataFrame API with SQL seamlessly. Register DataFrames as views and query with standard SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Register DataFrame as a temporary view ---\n",
    "df_employees.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# --- Simple SQL queries ---\n",
    "spark.sql(\"SELECT * FROM employees WHERE salary > 90000 ORDER BY salary DESC\").show()\n",
    "\n",
    "# --- Aggregations in SQL ---\n",
    "spark.sql(\"\"\"\n",
    "    SELECT department,\n",
    "           COUNT(*) as emp_count,\n",
    "           ROUND(AVG(salary), 2) as avg_salary,\n",
    "           MAX(salary) as max_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# --- CTE (Common Table Expression) ---\n",
    "spark.sql(\"\"\"\n",
    "    WITH ranked_employees AS (\n",
    "        SELECT name, department, salary,\n",
    "               ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rn\n",
    "        FROM employees\n",
    "    )\n",
    "    SELECT * FROM ranked_employees WHERE rn = 1\n",
    "\"\"\").show()\n",
    "\n",
    "# --- Subquery ---\n",
    "spark.sql(\"\"\"\n",
    "    SELECT name, salary, department\n",
    "    FROM employees\n",
    "    WHERE salary > (SELECT AVG(salary) FROM employees)\n",
    "    ORDER BY salary DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# --- Mix SQL results with DataFrame API ---\n",
    "top_earners = spark.sql(\"SELECT * FROM employees WHERE salary > 90000\")\n",
    "top_earners.groupBy(\"department\").count().show()\n",
    "\n",
    "# --- Using the UDF we registered earlier in SQL ---\n",
    "spark.sql(\"\"\"\n",
    "    SELECT name, salary, salary_band_sql(salary) as band\n",
    "    FROM employees\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a20ee19",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3 â€” ADVANCED\n",
    "---\n",
    "## 13. Reading & Writing Data in Delta Lake Format\n",
    "\n",
    "Delta Lake is the default storage format on Databricks. It provides ACID transactions, time travel, schema enforcement, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759bbafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Write DataFrame as Delta table ---\n",
    "delta_path = \"/tmp/pyspark_tutorial/employees_delta\"\n",
    "\n",
    "# Write in Delta format (overwrite if exists)\n",
    "df_employees.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "print(f\"âœ… Written to {delta_path}\")\n",
    "\n",
    "# --- Read Delta table ---\n",
    "df_delta = spark.read.format(\"delta\").load(delta_path)\n",
    "df_delta.show()\n",
    "\n",
    "# --- Write as a managed table ---\n",
    "df_employees.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"tutorial_employees\")\n",
    "spark.sql(\"SELECT * FROM tutorial_employees LIMIT 5\").show()\n",
    "\n",
    "# --- Read/Write other formats ---\n",
    "# CSV\n",
    "csv_path = \"/tmp/pyspark_tutorial/employees_csv\"\n",
    "df_employees.write.mode(\"overwrite\").option(\"header\", True).csv(csv_path)\n",
    "df_csv = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(csv_path)\n",
    "df_csv.show(3)\n",
    "\n",
    "# JSON\n",
    "json_path = \"/tmp/pyspark_tutorial/employees_json\"\n",
    "df_employees.write.mode(\"overwrite\").json(json_path)\n",
    "df_json = spark.read.json(json_path)\n",
    "df_json.show(3)\n",
    "\n",
    "# Parquet\n",
    "parquet_path = \"/tmp/pyspark_tutorial/employees_parquet\"\n",
    "df_employees.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "df_parquet = spark.read.parquet(parquet_path)\n",
    "df_parquet.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1df2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Delta Lake Advanced Features ---\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- MERGE / UPSERT: Update existing + insert new rows ---\n",
    "# New data with updates and new employees\n",
    "new_data = [\n",
    "    (\"Alice\", \"Engineering\", 100000, 31, \"2020-01-15\"),    # updated salary & age\n",
    "    (\"Liam\", \"Sales\", 88000, 27, \"2025-01-10\"),            # new employee\n",
    "    (\"Mia\", \"Engineering\", 97000, 29, \"2024-06-15\"),       # new employee\n",
    "]\n",
    "df_updates = spark.createDataFrame(new_data, columns)\n",
    "\n",
    "# Perform MERGE\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    df_updates.alias(\"source\"),\n",
    "    \"target.name = source.name\"\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "print(\"=== After MERGE ===\")\n",
    "spark.read.format(\"delta\").load(delta_path).show()\n",
    "\n",
    "# --- Time Travel: Query previous versions ---\n",
    "print(\"=== Version 0 (original data) ===\")\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path).show()\n",
    "\n",
    "# View Delta table history\n",
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)\n",
    "\n",
    "# --- Schema Evolution: Add new columns automatically ---\n",
    "df_with_bonus = df_employees.withColumn(\"bonus\", F.lit(5000))\n",
    "df_with_bonus.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "print(\"=== After Schema Evolution ===\")\n",
    "spark.read.format(\"delta\").load(delta_path).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba939c4",
   "metadata": {},
   "source": [
    "## 14. DataFrame Caching, Persistence & Performance Tuning\n",
    "\n",
    "Caching stores DataFrames in memory/disk to avoid recomputation. Use `explain()` to understand execution plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# --- cache(): Store DataFrame in memory ---\n",
    "df_cached = df_employees.cache()\n",
    "df_cached.count()  # Trigger caching (lazy evaluation)\n",
    "print(f\"Is cached: {df_cached.is_cached}\")\n",
    "\n",
    "# --- persist(): Choose storage level ---\n",
    "df_persisted = df_employees.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df_persisted.count()\n",
    "\n",
    "# --- unpersist(): Free the cached data ---\n",
    "df_cached.unpersist()\n",
    "df_persisted.unpersist()\n",
    "print(\"Cache cleared\")\n",
    "\n",
    "# --- explain(): View execution plan ---\n",
    "print(\"=== Simple Plan ===\")\n",
    "df_employees.filter(F.col(\"salary\") > 90000).select(\"name\", \"salary\").explain()\n",
    "\n",
    "print(\"\\n=== Extended Plan (Parsed â†’ Analyzed â†’ Optimized â†’ Physical) ===\")\n",
    "df_employees.filter(F.col(\"salary\") > 90000) \\\n",
    "    .groupBy(\"department\") \\\n",
    "    .agg(F.avg(\"salary\").alias(\"avg_salary\")) \\\n",
    "    .explain(mode=\"extended\")\n",
    "\n",
    "# --- Formatted plan (most readable in Databricks) ---\n",
    "print(\"\\n=== Formatted Plan ===\")\n",
    "df_employees.join(\n",
    "    df_employees.groupBy(\"department\").agg(F.avg(\"salary\").alias(\"dept_avg\")),\n",
    "    \"department\"\n",
    ").filter(F.col(\"salary\") > F.col(\"dept_avg\")).explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a25230c",
   "metadata": {},
   "source": [
    "## 15. Advanced Transformations with Higher-Order Functions\n",
    "\n",
    "Higher-order functions apply transformations directly on array columns without needing `explode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585cf76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with array columns\n",
    "df_scores = spark.createDataFrame([\n",
    "    (\"Alice\", [85, 92, 78, 95]),\n",
    "    (\"Bob\", [70, 65, 80, 72]),\n",
    "    (\"Charlie\", [90, 88, 95, 100]),\n",
    "], [\"name\", \"scores\"])\n",
    "\n",
    "# --- transform(): Apply function to each array element ---\n",
    "df_scores.select(\n",
    "    \"name\",\n",
    "    \"scores\",\n",
    "    F.transform(\"scores\", lambda x: x + 5).alias(\"scores_curved\"),  # Add 5 to each score\n",
    "    F.transform(\"scores\", lambda x: F.round(x / 100.0 * 4.0, 2)).alias(\"gpa_scale\"),\n",
    ").show(truncate=False)\n",
    "\n",
    "# --- filter(): Keep only elements matching condition ---\n",
    "df_scores.select(\n",
    "    \"name\",\n",
    "    \"scores\",\n",
    "    F.filter(\"scores\", lambda x: x >= 80).alias(\"passing_scores\"),\n",
    ").show(truncate=False)\n",
    "\n",
    "# --- aggregate(): Reduce array to single value ---\n",
    "df_scores.select(\n",
    "    \"name\",\n",
    "    \"scores\",\n",
    "    F.aggregate(\"scores\", F.lit(0), lambda acc, x: acc + x).alias(\"total_score\"),\n",
    "    F.aggregate(\n",
    "        \"scores\", F.lit(0),\n",
    "        lambda acc, x: acc + x,\n",
    "        lambda acc: F.round(acc / F.lit(4), 2)  # finalize: compute average\n",
    "    ).alias(\"avg_score\"),\n",
    ").show(truncate=False)\n",
    "\n",
    "# --- exists(): Check if any element matches ---\n",
    "df_scores.select(\n",
    "    \"name\",\n",
    "    F.exists(\"scores\", lambda x: x == 100).alias(\"has_perfect\"),\n",
    "    F.exists(\"scores\", lambda x: x < 70).alias(\"has_failing\"),\n",
    ").show()\n",
    "\n",
    "# --- forall(): Check if ALL elements match ---\n",
    "df_scores.select(\n",
    "    \"name\",\n",
    "    F.forall(\"scores\", lambda x: x >= 70).alias(\"all_passing\"),\n",
    ").show()\n",
    "\n",
    "# --- Chaining DataFrame transformations with .transform() ---\n",
    "def add_salary_band(df):\n",
    "    return df.withColumn(\"band\",\n",
    "        F.when(F.col(\"salary\") < 75000, \"Junior\")\n",
    "         .when(F.col(\"salary\") < 100000, \"Mid\")\n",
    "         .otherwise(\"Senior\"))\n",
    "\n",
    "def add_tax(df):\n",
    "    return df.withColumn(\"tax\", F.col(\"salary\") * 0.2)\n",
    "\n",
    "# Pipeline-style processing\n",
    "df_employees.transform(add_salary_band).transform(add_tax).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348d559",
   "metadata": {},
   "source": [
    "## 16. Broadcast Variables and Accumulators\n",
    "\n",
    "**Broadcast variables** efficiently share read-only data to all worker nodes. **Accumulators** are write-only variables for distributed counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Broadcast Variables ===\n",
    "# Useful for small lookup tables that every node needs\n",
    "dept_budgets = {\"Engineering\": 500000, \"Marketing\": 200000, \"HR\": 150000, \"Sales\": 300000}\n",
    "broadcast_budgets = spark.sparkContext.broadcast(dept_budgets)\n",
    "\n",
    "# Use broadcast variable in a UDF\n",
    "@udf(IntegerType())\n",
    "def get_budget(dept):\n",
    "    return broadcast_budgets.value.get(dept, 0)\n",
    "\n",
    "df_employees.select(\"name\", \"department\", get_budget(\"department\").alias(\"dept_budget\")).show()\n",
    "\n",
    "# === Broadcast Join (hint for small tables) ===\n",
    "# Forces Spark to broadcast the smaller table to avoid shuffle\n",
    "df_small_dept = spark.createDataFrame([\n",
    "    (\"Engineering\", \"Tech\"), (\"Marketing\", \"Business\"),\n",
    "    (\"HR\", \"Support\"), (\"Sales\", \"Business\")\n",
    "], [\"department\", \"category\"])\n",
    "\n",
    "df_broadcast_join = df_employees.join(\n",
    "    F.broadcast(df_small_dept),  # Hint: broadcast this small table\n",
    "    \"department\"\n",
    ")\n",
    "print(\"=== Broadcast Join ===\")\n",
    "df_broadcast_join.show()\n",
    "# Verify broadcast join in the plan\n",
    "df_broadcast_join.explain()\n",
    "\n",
    "# === Accumulators ===\n",
    "high_salary_count = spark.sparkContext.accumulator(0)\n",
    "total_salary_acc = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def process_row(row):\n",
    "    total_salary_acc.add(row.salary)\n",
    "    if row.salary > 90000:\n",
    "        high_salary_count.add(1)\n",
    "\n",
    "df_employees.foreach(process_row)\n",
    "print(f\"High salary employees (>90K): {high_salary_count.value}\")\n",
    "print(f\"Total salary sum: {total_salary_acc.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a89fe",
   "metadata": {},
   "source": [
    "## 17. Optimizing Spark Jobs: Partitioning, Bucketing & AQE\n",
    "\n",
    "Control data distribution for better performance. **Adaptive Query Execution (AQE)** is enabled by default in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa52f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Partitioning ===\n",
    "print(f\"Default partitions: {df_employees.rdd.getNumPartitions()}\")\n",
    "\n",
    "# repartition(): Increase partitions (full shuffle)\n",
    "df_repart = df_employees.repartition(4)\n",
    "print(f\"After repartition(4): {df_repart.rdd.getNumPartitions()}\")\n",
    "\n",
    "# repartition by column (great for joins/groupBy on that column)\n",
    "df_repart_col = df_employees.repartition(\"department\")\n",
    "print(f\"After repartition('department'): {df_repart_col.rdd.getNumPartitions()}\")\n",
    "\n",
    "# coalesce(): Reduce partitions (no full shuffle â€” more efficient than repartition for reducing)\n",
    "df_coalesced = df_repart.coalesce(2)\n",
    "print(f\"After coalesce(2): {df_coalesced.rdd.getNumPartitions()}\")\n",
    "\n",
    "# === Write with partitioning (partition pruning for queries) ===\n",
    "partitioned_path = \"/tmp/pyspark_tutorial/employees_partitioned\"\n",
    "df_employees.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"department\") \\\n",
    "    .save(partitioned_path)\n",
    "\n",
    "# Reading with partition pruning â€” only reads relevant partitions\n",
    "spark.read.format(\"delta\").load(partitioned_path) \\\n",
    "    .filter(F.col(\"department\") == \"Engineering\") \\\n",
    "    .explain()\n",
    "\n",
    "# === Adaptive Query Execution (AQE) â€” enabled by default in Databricks ===\n",
    "print(\"\\n=== AQE Configuration ===\")\n",
    "print(f\"AQE enabled: {spark.conf.get('spark.sql.adaptive.enabled', 'not set')}\")\n",
    "\n",
    "# Key AQE features (auto-managed in Databricks Serverless):\n",
    "# 1. Dynamically coalesces shuffle partitions\n",
    "# 2. Converts sort-merge joins to broadcast joins when data is small\n",
    "# 3. Optimizes skewed joins automatically\n",
    "\n",
    "# === Salting for skew joins (manual technique) ===\n",
    "# When one join key has disproportionately many rows\n",
    "import random\n",
    "\n",
    "# Add salt to distribute skewed key across partitions\n",
    "df_salted = df_employees.withColumn(\"salt\", (F.rand() * 4).cast(\"int\"))\n",
    "df_salted.select(\"name\", \"department\", \"salt\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e545640e",
   "metadata": {},
   "source": [
    "## 18. Structured Streaming Basics\n",
    "\n",
    "Structured Streaming processes data incrementally. Uses the same DataFrame API with `readStream`/`writeStream`.\n",
    "\n",
    "> **Note:** On Databricks free tier, streaming capabilities may be limited. The examples below use the built-in `rate` source for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651caad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Rate Source: Generates incrementing data for testing ===\n",
    "# Creates a stream with 'timestamp' and 'value' columns\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"rate\")               # Built-in test source\n",
    "    .option(\"rowsPerSecond\", 5)   # 5 rows per second\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Apply transformations on the stream (same API as batch)\n",
    "df_stream_transformed = (\n",
    "    df_stream\n",
    "    .withColumn(\"value_doubled\", F.col(\"value\") * 2)\n",
    "    .withColumn(\"is_even\", F.col(\"value\") % 2 == 0)\n",
    "    .withColumn(\"processed_at\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# === Write stream to memory (for testing in notebooks) ===\n",
    "query = (\n",
    "    df_stream_transformed\n",
    "    .writeStream\n",
    "    .format(\"memory\")             # Output to in-memory table\n",
    "    .queryName(\"rate_stream\")     # Table name to query\n",
    "    .outputMode(\"append\")         # append | complete | update\n",
    "    .trigger(processingTime=\"5 seconds\")  # Process every 5 seconds\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(f\"Stream is active: {query.isActive}\")\n",
    "print(f\"Stream status: {query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529305f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a few seconds then query the in-memory stream table\n",
    "import time\n",
    "time.sleep(10)\n",
    "\n",
    "# Query the streaming data like a regular table\n",
    "spark.sql(\"SELECT * FROM rate_stream ORDER BY timestamp DESC LIMIT 10\").show()\n",
    "\n",
    "# === Aggregation on streaming data (requires 'complete' output mode) ===\n",
    "query_agg = (\n",
    "    df_stream_transformed\n",
    "    .groupBy(\"is_even\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"count\"),\n",
    "        F.avg(\"value\").alias(\"avg_value\"),\n",
    "    )\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"rate_stream_agg\")\n",
    "    .outputMode(\"complete\")       # Required for aggregations\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "time.sleep(10)\n",
    "spark.sql(\"SELECT * FROM rate_stream_agg\").show()\n",
    "\n",
    "# === Monitor active streams ===\n",
    "print(f\"Active streams: {len(spark.streams.active)}\")\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"  - {stream.name}: {stream.status}\")\n",
    "\n",
    "# === Stop all streams (cleanup) ===\n",
    "for stream in spark.streams.active:\n",
    "    stream.stop()\n",
    "print(\"All streams stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfb7b4b",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Cleanup\n",
    "\n",
    "Remove temporary data created during this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cleanup temporary files and tables ---\n",
    "dbutils.fs.rm(\"/tmp/pyspark_tutorial\", recurse=True)\n",
    "spark.sql(\"DROP TABLE IF EXISTS tutorial_employees\")\n",
    "print(\"âœ… Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad2e76",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“š Quick Reference Cheat Sheet\n",
    "\n",
    "| **Category** | **Operation** | **Syntax** |\n",
    "|---|---|---|\n",
    "| **Create** | From list | `spark.createDataFrame(data, columns)` |\n",
    "| **Create** | With schema | `spark.createDataFrame(data, schema)` |\n",
    "| **Select** | Columns | `df.select(\"col1\", \"col2\")` |\n",
    "| **Filter** | Rows | `df.filter(F.col(\"x\") > 5)` |\n",
    "| **Sort** | Order | `df.orderBy(F.col(\"x\").desc())` |\n",
    "| **Aggregate** | GroupBy | `df.groupBy(\"col\").agg(F.sum(\"x\"))` |\n",
    "| **Join** | Inner | `df1.join(df2, \"key\", \"inner\")` |\n",
    "| **Window** | Rank | `F.row_number().over(window_spec)` |\n",
    "| **Null** | Fill | `df.na.fill({\"col\": \"default\"})` |\n",
    "| **Delta** | Write | `df.write.format(\"delta\").save(path)` |\n",
    "| **Delta** | Merge | `DeltaTable.forPath(spark, path).merge(...)` |\n",
    "| **SQL** | Query | `spark.sql(\"SELECT * FROM view\")` |\n",
    "| **Cache** | Memory | `df.cache()` |\n",
    "| **Stream** | Read | `spark.readStream.format(\"rate\").load()` |\n",
    "| **Debug** | Plan | `df.explain(mode=\"formatted\")` |\n",
    "\n",
    "---\n",
    "**Happy Learning! ðŸŽ‰**  \n",
    "*This notebook is compatible with Databricks Community Edition (Free) and Databricks Serverless Compute.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
