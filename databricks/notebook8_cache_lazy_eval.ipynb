{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cache, Persist, and Lazy Evaluation in PySpark\n",
        "\n",
        "This notebook focuses on **fundamental Spark concepts** for students:\n",
        "\n",
        "Concepts:\n",
        "1. Transformations vs Actions (lazy evaluation)\n",
        "2. `cache()` and `persist()`\n",
        "3. When caching helps\n",
        "4. `unpersist()`\n",
        "5. Viewing the execution plan with `explain()`\n",
        "\n",
        "Dataset used: `samples.tpch.orders`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "import time\n",
        "\n",
        "orders_df = spark.read.table(\"samples.tpch.orders\")\n",
        "print(\"Orders count:\", orders_df.count())\n",
        "display(orders_df.limit(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Transformations vs Actions\n",
        "\n",
        "- **Transformations**: define a recipe (no work is done yet).  \n",
        "  Examples: `select`, `filter`, `withColumn`, `groupBy`, `repartition`.\n",
        "- **Actions**: actually run the recipe and return a result.  \n",
        "  Examples: `count`, `show`, `collect`, `take`, `display`.\n",
        "\n",
        "Spark uses **lazy evaluation**:\n",
        "- It waits until an action is called to build a plan and execute it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Only transformations (no action yet)\n",
        "filtered_orders = orders_df.filter(F.col(\"o_totalprice\") > 10000)\n",
        "projected_orders = filtered_orders.select(\"o_orderkey\", \"o_custkey\", \"o_totalprice\")\n",
        "\n",
        "# No job has run yet. We only defined a *plan*.\n",
        "print(\"We defined transformations but haven't triggered any action yet.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Now an action: count()\n",
        "start = time.time()\n",
        "count_val = projected_orders.count()\n",
        "end = time.time()\n",
        "\n",
        "print(\"Count:\", count_val)\n",
        "print(\"Time for first count (no cache):\", round(end - start, 3), \"seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Using `cache()`\n",
        "\n",
        "If we know we'll reuse the same DataFrame multiple times, we can **cache** it.\n",
        "\n",
        "- `df.cache()` tells Spark to keep the data in memory after the first action.\n",
        "- The **first** action still does full work.\n",
        "- Subsequent actions are usually faster.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "cached_orders = projected_orders.cache()\n",
        "\n",
        "# First action (materializes the cache)\n",
        "start = time.time()\n",
        "_ = cached_orders.count()\n",
        "end = time.time()\n",
        "\n",
        "print(\"Time for count with cache (first time):\", round(end - start, 3), \"seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Second action on the same cached DataFrame\n",
        "start = time.time()\n",
        "max_price = cached_orders.agg(F.max(\"o_totalprice\").alias(\"max_price\")).collect()[0][\"max_price\"]\n",
        "end = time.time()\n",
        "\n",
        "print(\"Max price:\", max_price)\n",
        "print(\"Time for second action on cached data:\", round(end - start, 3), \"seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. `persist()` with Storage Levels\n",
        "\n",
        "`cache()` is shorthand for `persist(StorageLevel.MEMORY_AND_DISK)`.\n",
        "\n",
        "We can choose other levels:\n",
        "- MEMORY_ONLY\n",
        "- MEMORY_AND_DISK\n",
        "- DISK_ONLY\n",
        "- etc.\n",
        "\n",
        "For teaching purposes, we\u2019ll show syntax only (behavior may depend on cluster size).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark import StorageLevel\n",
        "\n",
        "# Example: force DISK_ONLY (just for demonstration)\n",
        "disk_persisted = projected_orders.persist(StorageLevel.DISK_ONLY)\n",
        "\n",
        "print(\"Storage level:\", disk_persisted.storageLevel)\n",
        "\n",
        "# Trigger materialization\n",
        "_ = disk_persisted.count()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. `unpersist()`\n",
        "\n",
        "When you are done with a cached/persisted DataFrame, you should `unpersist()` it:\n",
        "\n",
        "- Frees up memory / disk used by cache\n",
        "- Good practice in long-running notebooks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Unpersist both\n",
        "cached_orders.unpersist()\n",
        "disk_persisted.unpersist()\n",
        "\n",
        "print(\"Unpersisted cached DataFrames.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Viewing the Execution Plan with `explain()`\n",
        "\n",
        "`explain()` shows:\n",
        "- Logical and physical plans\n",
        "- Where filters, scans, and shuffles happen\n",
        "\n",
        "Let\u2019s inspect the plan for a small aggregation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "agg_df = (\n",
        "    orders_df\n",
        "    .filter(F.col(\"o_orderstatus\") == \"F\")\n",
        "    .groupBy(\"o_orderpriority\")\n",
        "    .agg(F.avg(\"o_totalprice\").alias(\"avg_price\"))\n",
        ")\n",
        "\n",
        "agg_df.explain(mode=\"extended\")\n",
        "display(agg_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary for Students\n",
        "\n",
        "- Spark is **lazy**: transformations build a plan; actions trigger execution.\n",
        "- Use `cache()`/`persist()` when:\n",
        "  - The same DataFrame is used in **multiple actions**.\n",
        "  - The computation is **expensive**.\n",
        "- Always `unpersist()` when cached data is no longer needed.\n",
        "- Use `explain()` to understand what Spark is doing under the hood.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}