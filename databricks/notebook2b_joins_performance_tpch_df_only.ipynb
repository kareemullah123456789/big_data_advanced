{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Join Strategies & Performance Tuning with PySpark (DataFrame-only, Serverless-friendly)\n",
        "\n",
        "**Datasets:**\n",
        "- `samples.tpch.customer`\n",
        "- `samples.tpch.orders`\n",
        "- `samples.tpch.lineitem`\n",
        "\n",
        "In this notebook you will:\n",
        "1. Perform star-schema joins\n",
        "2. Inspect physical plans (`explain`)\n",
        "3. Use broadcast joins\n",
        "4. Use caching & reuse\n",
        "5. Use `repartition` / `coalesce` with **DataFrame-only partition introspection**\n",
        "6. Enable Adaptive Query Execution (AQE)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "customer_df = spark.read.table(\"samples.tpch.customer\")\n",
        "orders_df   = spark.read.table(\"samples.tpch.orders\")\n",
        "lineitem_df = spark.read.table(\"samples.tpch.lineitem\")\n",
        "\n",
        "print(\"Customer count:\", customer_df.count())\n",
        "print(\"Orders count:\", orders_df.count())\n",
        "print(\"Lineitem count:\", lineitem_df.count())\n",
        "\n",
        "display(customer_df.limit(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Basic Star-Schema Join\n",
        "\n",
        "We'll join:\n",
        "- `customer` -> `orders` on `c_custkey = o_custkey`\n",
        "- `orders` -> `lineitem` on `o_orderkey = l_orderkey`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Join customer to orders\n",
        "cust_orders_df = (\n",
        "    customer_df.alias(\"c\")\n",
        "    .join(orders_df.alias(\"o\"), F.col(\"c.c_custkey\") == F.col(\"o.o_custkey\"), \"inner\")\n",
        ")\n",
        "\n",
        "# Join the result to lineitem\n",
        "cust_orders_lineitem_df = (\n",
        "    cust_orders_df.alias(\"co\")\n",
        "    .join(lineitem_df.alias(\"l\"), F.col(\"co.o_orderkey\") == F.col(\"l.l_orderkey\"), \"inner\")\n",
        ")\n",
        "\n",
        "display(cust_orders_lineitem_df.select(\"c_custkey\", \"o_orderkey\", \"l_linenumber\").limit(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Inspect the Physical Plan with `explain`\n",
        "\n",
        "This shows:\n",
        "- Join types (BroadcastHashJoin, SortMergeJoin, etc.)\n",
        "- Shuffle operations\n",
        "- Estimated statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "cust_orders_lineitem_df.explain(mode=\"extended\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Aggregate Query as a Baseline\n",
        "\n",
        "Example query:\n",
        "- Revenue per customer (`c_custkey`)\n",
        "- Using sum of `l_extendedprice * (1 - l_discount)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "baseline_revenue_df = (\n",
        "    cust_orders_lineitem_df\n",
        "    .groupBy(\"c_custkey\")\n",
        "    .agg(\n",
        "        F.sum(\n",
        "            F.col(\"l_extendedprice\") * (1 - F.col(\"l_discount\"))\n",
        "        ).alias(\"customer_revenue\")\n",
        "    )\n",
        ")\n",
        "\n",
        "display(baseline_revenue_df.orderBy(F.col(\"customer_revenue\").desc()).limit(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Broadcast Join Optimization\n",
        "\n",
        "- If one side of a join is **small enough**, we can broadcast it.\n",
        "- Spark then avoids a shuffle on that side.\n",
        "\n",
        "We'll:\n",
        "- Broadcast the `customer` table when joining to `orders`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "broadcast_cust_orders_df = (\n",
        "    broadcast(customer_df.alias(\"c\"))\n",
        "    .join(orders_df.alias(\"o\"), F.col(\"c.c_custkey\") == F.col(\"o.o_custkey\"), \"inner\")\n",
        ")\n",
        "\n",
        "broadcast_all_df = (\n",
        "    broadcast_cust_orders_df.alias(\"co\")\n",
        "    .join(lineitem_df.alias(\"l\"), F.col(\"co.o_orderkey\") == F.col(\"l.l_orderkey\"), \"inner\")\n",
        ")\n",
        "\n",
        "broadcast_all_df.explain(mode=\"extended\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Caching & Reuse\n",
        "\n",
        "If you use the same intermediate result many times:\n",
        "- Use `.cache()` or `.persist()` to avoid recompute + re-read.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cache the heavy join\n",
        "broadcast_all_df_cached = broadcast_all_df.cache()\n",
        "\n",
        "# Trigger cache materialization\n",
        "broadcast_all_df_cached.count()\n",
        "\n",
        "# Re-use cached DF for multiple aggregations\n",
        "revenue_by_customer_df = (\n",
        "    broadcast_all_df_cached\n",
        "    .groupBy(\"c_custkey\")\n",
        "    .agg(\n",
        "        F.sum(\n",
        "            F.col(\"l_extendedprice\") * (1 - F.col(\"l_discount\"))\n",
        "        ).alias(\"customer_revenue\")\n",
        "    )\n",
        ")\n",
        "\n",
        "revenue_by_nation_df = (\n",
        "    broadcast_all_df_cached\n",
        "    .groupBy(\"c_nationkey\")\n",
        "    .agg(\n",
        "        F.sum(\n",
        "            F.col(\"l_extendedprice\") * (1 - F.col(\"l_discount\"))\n",
        "        ).alias(\"nation_revenue\")\n",
        "    )\n",
        ")\n",
        "\n",
        "display(revenue_by_customer_df.orderBy(F.col(\"customer_revenue\").desc()).limit(10))\n",
        "display(revenue_by_nation_df.orderBy(F.col(\"nation_revenue\").desc()).limit(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Repartitioning & Coalescing (DataFrame-only Partition Introspection)\n",
        "\n",
        "- Use `repartition()` to **increase** parallelism or shuffle by keys.\n",
        "- Use `coalesce()` to **decrease** number of partitions without a full shuffle.\n",
        "- Instead of `df.rdd.getNumPartitions()`, we use `spark_partition_id()` to count partitions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Repartition by key used in downstream aggregations\n",
        "repartitioned_df = broadcast_all_df.repartition(64, \"c_custkey\")  # 64 is just an example\n",
        "\n",
        "repartitioned_with_pid = repartitioned_df.withColumn(\"partition_id\", F.spark_partition_id())\n",
        "num_parts_repart = (\n",
        "    repartitioned_with_pid\n",
        "    .select(\"partition_id\")\n",
        "    .agg(F.countDistinct(\"partition_id\").alias(\"num_partitions\"))\n",
        "    .collect()[0][\"num_partitions\"]\n",
        ")\n",
        "\n",
        "print(\"Repartitioned partitions (via spark_partition_id):\", num_parts_repart)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Coalesce when writing out or for subsequent stages\n",
        "coalesced_df = repartitioned_df.coalesce(8)\n",
        "coalesced_with_pid = coalesced_df.withColumn(\"partition_id\", F.spark_partition_id())\n",
        "num_parts_coal = (\n",
        "    coalesced_with_pid\n",
        "    .select(\"partition_id\")\n",
        "    .agg(F.countDistinct(\"partition_id\").alias(\"num_partitions\"))\n",
        "    .collect()[0][\"num_partitions\"]\n",
        ")\n",
        "\n",
        "print(\"Coalesced partitions (via spark_partition_id):\", num_parts_coal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Adaptive Query Execution (AQE)\n",
        "\n",
        "AQE can:\n",
        "- Automatically coalesce shuffle partitions\n",
        "- Change join strategies at runtime\n",
        "- Handle skew\n",
        "\n",
        "Make sure it's enabled:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
        "\n",
        "print(\"AQE enabled:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
        "\n",
        "# Run an aggregation to let AQE optimize it\n",
        "aqe_example_df = (\n",
        "    broadcast_all_df\n",
        "    .groupBy(\"c_custkey\")\n",
        "    .agg(F.sum(\"l_extendedprice\").alias(\"total_extended_price\"))\n",
        ")\n",
        "\n",
        "aqe_example_df.explain(mode=\"extended\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}