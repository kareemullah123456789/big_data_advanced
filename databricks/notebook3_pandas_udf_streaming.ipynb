{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pandas UDFs & Structured Streaming with PySpark\n",
        "\n",
        "In this notebook you will:\n",
        "1. Compare normal UDFs vs built-in functions vs Pandas UDFs\n",
        "2. Use Pandas UDFs on `samples.nyctaxi.trips`\n",
        "3. Build a basic Structured Streaming pipeline (rate source)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "import pandas as pd\n",
        "\n",
        "nyc_taxi_df = spark.read.table(\"samples.nyctaxi.trips\")\n",
        "display(nyc_taxi_df.limit(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Normal Python UDF vs Built-in vs Pandas UDF\n",
        "\n",
        "We'll compute a simple custom metric:\n",
        "\n",
        "```python\n",
        "score = log(1 + fare_amount) * (1 + tip_rate)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Normal Python UDF (row-by-row, slower)\n",
        "def tip_score_py(fare_amount, tip_amount):\n",
        "    if fare_amount is None or fare_amount <= 0:\n",
        "        return None\n",
        "    tip_rate = tip_amount / fare_amount if tip_amount is not None else 0.0\n",
        "    return math.log1p(fare_amount) * (1 + tip_rate)\n",
        "\n",
        "tip_score_udf = F.udf(tip_score_py, DoubleType())\n",
        "\n",
        "sample_df = nyc_taxi_df.select(\"fare_amount\", \"tip_amount\").limit(10000)\n",
        "\n",
        "py_udf_df = sample_df.withColumn(\"tip_score_py_udf\", tip_score_udf(\"fare_amount\", \"tip_amount\"))\n",
        "display(py_udf_df.limit(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Pandas UDF (Vectorized)\n",
        "\n",
        "- Operates on **Pandas Series** instead of single rows\n",
        "- Often much faster for heavy Python logic\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "@pandas_udf(\"double\")\n",
        "def tip_score_pandas(fare_amount: pd.Series, tip_amount: pd.Series) -> pd.Series:\n",
        "    fare = fare_amount.fillna(0.0)\n",
        "    tip = tip_amount.fillna(0.0)\n",
        "    tip_rate = tip / fare.replace(0.0, pd.NA)\n",
        "    tip_rate = tip_rate.fillna(0.0)\n",
        "    return pd.Series([math.log1p(f) * (1 + r) for f, r in zip(fare, tip_rate)])\n",
        "\n",
        "pandas_udf_df = sample_df.withColumn(\"tip_score_pandas\", tip_score_pandas(\"fare_amount\", \"tip_amount\"))\n",
        "\n",
        "display(pandas_udf_df.limit(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Comparing with Built-in Functions\n",
        "\n",
        "If you can avoid UDFs, always prefer built-ins.\n",
        "\n",
        "We'll implement an **approximate** version of the score using built-ins only:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "builtin_df = (\n",
        "    sample_df\n",
        "    .withColumn(\n",
        "        \"tip_rate\",\n",
        "        F.when(F.col(\"fare_amount\") > 0,\n",
        "               F.col(\"tip_amount\") / F.col(\"fare_amount\")).otherwise(0.0)\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"tip_score_builtin\",\n",
        "        F.log1p(\"fare_amount\") * (1 + F.col(\"tip_rate\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "display(builtin_df.limit(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Window + Pandas UDF Example\n",
        "\n",
        "We'll:\n",
        "- Use a Pandas UDF with **grouped map** to compute custom stats per `passenger_count`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
        "\n",
        "# Schema for grouped-map Pandas UDF\n",
        "schema = StructType([\n",
        "    StructField(\"passenger_count\", IntegerType()),\n",
        "    StructField(\"avg_fare\", DoubleType()),\n",
        "    StructField(\"std_fare\", DoubleType()),\n",
        "    StructField(\"avg_tip_rate\", DoubleType())\n",
        "])\n",
        "\n",
        "@pandas_udf(schema, functionType=\"grouped_map\")\n",
        "def passenger_stats(pdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    pc = int(pdf[\"passenger_count\"].iloc[0])\n",
        "    avg_fare = pdf[\"fare_amount\"].mean()\n",
        "    std_fare = pdf[\"fare_amount\"].std()\n",
        "    # Handle division by zero\n",
        "    valid = pdf[\"fare_amount\"] > 0\n",
        "    tip_rate = (pdf.loc[valid, \"tip_amount\"] / pdf.loc[valid, \"fare_amount\"]).fillna(0.0)\n",
        "    avg_tip_rate = tip_rate.mean()\n",
        "    return pd.DataFrame([{\n",
        "        \"passenger_count\": pc,\n",
        "        \"avg_fare\": avg_fare,\n",
        "        \"std_fare\": std_fare,\n",
        "        \"avg_tip_rate\": avg_tip_rate\n",
        "    }])\n",
        "\n",
        "stats_df = (\n",
        "    nyc_taxi_df\n",
        "    .select(\"passenger_count\", \"fare_amount\", \"tip_amount\")\n",
        "    .groupby(\"passenger_count\")\n",
        "    .apply(passenger_stats)\n",
        ")\n",
        "\n",
        "display(stats_df.orderBy(\"passenger_count\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Structured Streaming Example (Rate Source)\n",
        "\n",
        "We'll:\n",
        "- Use the `rate` source to generate a stream of rows\n",
        "- Compute a moving count + average over time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create a streaming DataFrame with 10 rows per second\n",
        "stream_df = (\n",
        "    spark.readStream\n",
        "         .format(\"rate\")\n",
        "         .option(\"rowsPerSecond\", 10)\n",
        "         .load()\n",
        ")\n",
        "\n",
        "display(stream_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Transform the Stream\n",
        "\n",
        "We'll:\n",
        "- Group by 10-second windows\n",
        "- Compute count and average of `value`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "agg_stream_df = (\n",
        "    stream_df\n",
        "    .groupBy(\n",
        "        F.window(F.col(\"timestamp\"), \"10 seconds\").alias(\"time_window\")\n",
        "    )\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"rows_in_window\"),\n",
        "        F.avg(\"value\").alias(\"avg_value\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# For debugging in Databricks, write to the memory sink\n",
        "query = (\n",
        "    agg_stream_df.writeStream\n",
        "    .format(\"memory\")\n",
        "    .queryName(\"rate_agg\")\n",
        "    .outputMode(\"complete\")\n",
        "    .trigger(processingTime=\"5 seconds\")  # micro-batch every 5s\n",
        "    .start()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Query the In-Memory Table\n",
        "\n",
        "Run this cell multiple times to see streaming updates.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "display(spark.sql(\"SELECT * FROM rate_agg ORDER BY time_window\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Stop the Stream When Done\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Stop the query to avoid leaving a stream running\n",
        "query.stop()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}