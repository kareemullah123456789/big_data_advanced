{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Delta Lake Basics with PySpark \u2013 Upserts & Time Travel\n",
        "\n",
        "**Base dataset**: `samples.nyctaxi.trips`  \n",
        "**Target**: a small Delta table in your workspace (in DBFS)\n",
        "\n",
        "In this notebook you will:\n",
        "1. Create a small Delta table from NYC Taxi data\n",
        "2. Perform INSERT and UPDATE operations\n",
        "3. Use `MERGE INTO` for upserts\n",
        "4. Explore Delta time travel (view older versions of data)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "# Path in DBFS for demo Delta table (adjust as needed)\n",
        "delta_path = \"/tmp/demo_nyctaxi_delta\"\n",
        "\n",
        "# Load base data\n",
        "nyc_taxi_df = spark.read.table(\"samples.nyctaxi.trips\")\n",
        "\n",
        "# For demo purposes, take a small subset\n",
        "base_df = (\n",
        "    nyc_taxi_df\n",
        "    .select(\"vendor_id\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
        "            \"passenger_count\", \"trip_distance\", \"fare_amount\")\n",
        "    .limit(10000)\n",
        ")\n",
        "\n",
        "display(base_df.limit(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Write Initial Delta Table\n",
        "\n",
        "We will:\n",
        "- Overwrite any existing data at `delta_path`\n",
        "- Save as a Delta table\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Clean up any previous run\n",
        "dbutils.fs.rm(delta_path, recurse=True)\n",
        "\n",
        "(\n",
        "    base_df\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")\n",
        "    .save(delta_path)\n",
        ")\n",
        "\n",
        "# Read it back as a Delta table\n",
        "delta_df = spark.read.format(\"delta\").load(delta_path)\n",
        "print(\"Initial row count:\", delta_df.count())\n",
        "display(delta_df.limit(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Convert Path to a `DeltaTable` Object\n",
        "\n",
        "`DeltaTable` provides programmatic APIs for:\n",
        "- MERGE\n",
        "- UPDATE\n",
        "- DELETE\n",
        "- History\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "delta_table = DeltaTable.forPath(spark, delta_path)\n",
        "\n",
        "# Show Delta table history\n",
        "delta_table.history().show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Simple UPDATE Operation\n",
        "\n",
        "Example:\n",
        "- Flag \"long\" trips where `trip_distance >= 10`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Add a new boolean column via UPDATE\n",
        "delta_table.update(\n",
        "    condition=\"trip_distance >= 10\",\n",
        "    set={\"is_long_trip\": \"true\"}\n",
        ")\n",
        "\n",
        "# Read and confirm\n",
        "updated_df = spark.read.format(\"delta\").load(delta_path)\n",
        "display(updated_df.limit(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. MERGE INTO \u2013 Upsert Scenario\n",
        "\n",
        "We'll simulate:\n",
        "- A small set of updated rows for a given vendor\n",
        "- New rows that do not yet exist\n",
        "\n",
        "We use `MERGE` to:\n",
        "- UPDATE matching records\n",
        "- INSERT non-matching records\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Create a small DataFrame with updates & new rows\n",
        "sample_existing = (\n",
        "    updated_df\n",
        "    .filter(\"vendor_id IS NOT NULL\")\n",
        "    .limit(3)\n",
        "    .collect()\n",
        ")\n",
        "\n",
        "rows = []\n",
        "for r in sample_existing:\n",
        "    rows.append(Row(\n",
        "        vendor_id=r[\"vendor_id\"],\n",
        "        tpep_pickup_datetime=r[\"tpep_pickup_datetime\"],\n",
        "        tpep_dropoff_datetime=r[\"tpep_dropoff_datetime\"],\n",
        "        passenger_count=r[\"passenger_count\"],\n",
        "        trip_distance=r[\"trip_distance\"],\n",
        "        fare_amount=r[\"fare_amount\"] + 5.0,\n",
        "        is_long_trip=r.get(\"is_long_trip\", None)\n",
        "    ))\n",
        "\n",
        "# Add a brand-new row (non-matching key)\n",
        "rows.append(Row(\n",
        "    vendor_id=999,\n",
        "    tpep_pickup_datetime=updated_df.select(F.min(\"tpep_pickup_datetime\")).first()[0],\n",
        "    tpep_dropoff_datetime=updated_df.select(F.max(\"tpep_dropoff_datetime\")).first()[0],\n",
        "    passenger_count=1,\n",
        "    trip_distance=3.0,\n",
        "    fare_amount=25.0,\n",
        "    is_long_trip=False\n",
        "))\n",
        "\n",
        "updates_df = spark.createDataFrame(rows)\n",
        "\n",
        "print(\"Updates + inserts:\")\n",
        "display(updates_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create a DeltaTable object again\n",
        "delta_table = DeltaTable.forPath(spark, delta_path)\n",
        "\n",
        "# Define a unique key for merge: (vendor_id, pickup, dropoff)\n",
        "merge_condition = '''\n",
        "  t.vendor_id = u.vendor_id AND\n",
        "  t.tpep_pickup_datetime = u.tpep_pickup_datetime AND\n",
        "  t.tpep_dropoff_datetime = u.tpep_dropoff_datetime\n",
        "'''\n",
        "\n",
        "(\n",
        "    delta_table.alias(\"t\")\n",
        "    .merge(\n",
        "        updates_df.alias(\"u\"),\n",
        "        merge_condition\n",
        "    )\n",
        "    .whenMatchedUpdateAll()\n",
        "    .whenNotMatchedInsertAll()\n",
        "    .execute()\n",
        ")\n",
        "\n",
        "print(\"After MERGE, row count:\", spark.read.format(\"delta\").load(delta_path).count())\n",
        "delta_table.history().show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Delta Time Travel\n",
        "\n",
        "Delta keeps a transaction log with versions.\n",
        "\n",
        "You can query older versions using:\n",
        "- `versionAsOf`\n",
        "- `timestampAsOf`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Get full history\n",
        "hist_df = delta_table.history()\n",
        "display(hist_df)\n",
        "\n",
        "versions = [row[\"version\"] for row in hist_df.collect()]\n",
        "print(\"Available versions:\", versions)\n",
        "\n",
        "# Read current (latest) version\n",
        "current_df = spark.read.format(\"delta\").load(delta_path)\n",
        "print(\"Current version row count:\", current_df.count())\n",
        "\n",
        "# Read an older version (e.g., earliest)\n",
        "older_version = min(versions)\n",
        "old_df = (\n",
        "    spark.read\n",
        "         .format(\"delta\")\n",
        "         .option(\"versionAsOf\", older_version)\n",
        "         .load(delta_path)\n",
        ")\n",
        "\n",
        "print(f\"Old version ({older_version}) row count:\", old_df.count())\n",
        "display(old_df.limit(5))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}