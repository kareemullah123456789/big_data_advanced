{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Writing Partitioned Data, `repartition`, and `coalesce`\n",
        "\n",
        "This notebook shows:\n",
        "1. How to write data to files\n",
        "2. How `partitionBy` works when writing\n",
        "3. How `repartition` / `coalesce` affect number of output files\n",
        "4. Using a built-in dataset: `samples.nyctaxi.trips`\n",
        "\n",
        "> \u26a0\ufe0f These examples write to `/tmp/...` in DBFS.  \n",
        "> You can safely delete those folders later with `dbutils.fs.rm(path, recurse=True)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "nyc_df = spark.read.table(\"samples.nyctaxi.trips\")\n",
        "\n",
        "print(\"Total rows:\", nyc_df.count())\n",
        "display(nyc_df.limit(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Writing a Small Sample Without Partitioning\n",
        "\n",
        "We will:\n",
        "- Take a small subset\n",
        "- Write it as Parquet to `/tmp/student_nyc_basic`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Path for basic write (adjust if needed)\n",
        "path_basic = \"/tmp/student_nyc_basic\"\n",
        "\n",
        "# Clean up old data if it exists\n",
        "dbutils.fs.rm(path_basic, recurse=True)\n",
        "\n",
        "sample_df = nyc_df.select(\n",
        "    \"vendor_id\",\n",
        "    \"passenger_count\",\n",
        "    \"trip_distance\",\n",
        "    \"fare_amount\"\n",
        ").limit(5000)\n",
        "\n",
        "(\n",
        "    sample_df\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .parquet(path_basic)\n",
        ")\n",
        "\n",
        "print(\"Files written (no partitionBy):\")\n",
        "display(dbutils.fs.ls(path_basic))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each file inside the folder is a **partition file**.\n",
        "\n",
        "By default:\n",
        "- Spark decides the number of output files based on the number of **partitions** of the DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Writing Partitioned Data with `partitionBy`\n",
        "\n",
        "We will:\n",
        "- Write the same sample, but partition by `passenger_count`.\n",
        "- Each distinct `passenger_count` will have its **own folder**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "path_part = \"/tmp/student_nyc_partitioned\"\n",
        "\n",
        "dbutils.fs.rm(path_part, recurse=True)\n",
        "\n",
        "(\n",
        "    sample_df\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .partitionBy(\"passenger_count\")\n",
        "    .parquet(path_part)\n",
        ")\n",
        "\n",
        "print(\"Top-level folders when partitioned by passenger_count:\")\n",
        "display(dbutils.fs.ls(path_part))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Inspect one of the partition folders\n",
        "# (change the passenger_count value based on what's shown above)\n",
        "# Example for passenger_count=1:\n",
        "example_folder = path_part + \"/passenger_count=1\"\n",
        "print(\"Files in one partition folder:\")\n",
        "display(dbutils.fs.ls(example_folder))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When using `partitionBy`:\n",
        "- Spark writes subfolders like `passenger_count=1`, `passenger_count=2`, etc.\n",
        "- This makes **filtering** by that column very efficient (Spark can skip whole folders).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. `repartition` and `coalesce` Before Writing\n",
        "\n",
        "We can control the number of output files by changing the number of partitions **before write**:\n",
        "- `repartition(n)` \u2192 shuffle to exactly **n** partitions.\n",
        "- `coalesce(n)` \u2192 reduce to **n** partitions without full shuffle (best when you are decreasing partitions).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "path_repart = \"/tmp/student_nyc_repartitioned\"\n",
        "\n",
        "dbutils.fs.rm(path_repart, recurse=True)\n",
        "\n",
        "# Example: repartition to 8 partitions, then write\n",
        "sample_repart = sample_df.repartition(8)\n",
        "\n",
        "print(\"Partitions before write:\", sample_repart.rdd.getNumPartitions())\n",
        "\n",
        "(\n",
        "    sample_repart\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .parquet(path_repart)\n",
        ")\n",
        "\n",
        "print(\"Files written after repartition(8):\")\n",
        "display(dbutils.fs.ls(path_repart))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "path_coalesce = \"/tmp/student_nyc_coalesced\"\n",
        "\n",
        "dbutils.fs.rm(path_coalesce, recurse=True)\n",
        "\n",
        "# Example: coalesce down to 2 partitions, then write\n",
        "sample_coal = sample_df.coalesce(2)\n",
        "\n",
        "print(\"Partitions before write:\", sample_coal.rdd.getNumPartitions())\n",
        "\n",
        "(\n",
        "    sample_coal\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .parquet(path_coalesce)\n",
        ")\n",
        "\n",
        "print(\"Files written after coalesce(2):\")\n",
        "display(dbutils.fs.ls(path_coalesce))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Summary for Students\n",
        "\n",
        "- When you **write** a DataFrame, each partition usually becomes **one file**.\n",
        "- `partitionBy(col)` creates **subfolders** based on column values (great for filtering).\n",
        "- Use `repartition(n)` to change to a specific number of partitions **with a shuffle**.\n",
        "- Use `coalesce(n)` to **reduce** partitions with minimal shuffling.\n",
        "- Too many small files is bad (overhead). Too few very large files can reduce parallelism.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}