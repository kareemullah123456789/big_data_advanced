{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Partitions, \"glom\"-style view, Narrow vs Wide (DataFrame-only)\n",
        "\n",
        "This version avoids using the **RDD API**, so it works better on **serverless** environments\n",
        "where only DataFrame / SQL operations are supported.\n",
        "\n",
        "We will use:\n",
        "- `spark.range()` (built-in) for tiny examples\n",
        "- `samples.nyctaxi.trips` for a real dataset\n",
        "\n",
        "Concepts:\n",
        "1. What is a partition?\n",
        "2. How to inspect partitions with `spark_partition_id()`\n",
        "3. A DataFrame-only \"glom\" style view\n",
        "4. Narrow vs Wide transformations\n",
        "5. `repartition` vs `coalesce` (DataFrame API)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Tiny DataFrame with numbers 0..19\n",
        "df = spark.range(0, 20).toDF(\"id\")\n",
        "\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is a partition? (DataFrame-only)\n",
        "\n",
        "- A **partition** is a chunk of your data.\n",
        "- Spark processes data partition by partition.\n",
        "- We can see which row is in which partition with the built-in function:\n",
        "  `spark_partition_id()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Add a column showing the partition id for each row\n",
        "df_with_pid = df.withColumn(\"partition_id\", F.spark_partition_id())\n",
        "\n",
        "display(df_with_pid.orderBy(\"partition_id\", \"id\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Count how many partitions we have (DataFrame-only)\n",
        "num_partitions = (\n",
        "    df_with_pid\n",
        "    .select(F.spark_partition_id().alias(\"pid\"))\n",
        "    .agg(F.countDistinct(\"pid\").alias(\"num_partitions\"))\n",
        "    .collect()[0][\"num_partitions\"]\n",
        ")\n",
        "\n",
        "print(\"Number of partitions (DataFrame-only):\", num_partitions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. \"glom\"-style view without RDD\n",
        "\n",
        "In RDD API, `glom()` shows the list of elements in each partition.\n",
        "\n",
        "We can mimic this with DataFrame operations:\n",
        "- Add `partition_id` with `spark_partition_id()`\n",
        "- Group by `partition_id`\n",
        "- Collect rows as a list\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "glom_like = (\n",
        "    df_with_pid\n",
        "    .groupBy(\"partition_id\")\n",
        "    .agg(F.collect_list(\"id\").alias(\"rows_in_partition\"))\n",
        "    .orderBy(\"partition_id\")\n",
        ")\n",
        "\n",
        "display(glom_like)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Narrow vs Wide (Concepts Recap)\n",
        "\n",
        "**Narrow transformations**:\n",
        "- Each input partition contributes to **only one** output partition.\n",
        "- Examples: `select`, `filter`, `withColumn` (most simple column ops).\n",
        "- No shuffle of data between machines.\n",
        "\n",
        "**Wide transformations**:\n",
        "- Input data may be **redistributed** across many partitions.\n",
        "- Examples: `groupBy`, `distinct`, `join`, `repartition`.\n",
        "- These cause a **shuffle** (expensive).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Example of NARROW transformations\n",
        "narrow_df = (\n",
        "    df\n",
        "    .withColumn(\"id_times_2\", F.col(\"id\") * 2)  # withColumn = narrow\n",
        "    .filter(F.col(\"id\") % 2 == 0)              # filter = narrow\n",
        "    .withColumn(\"partition_id\", F.spark_partition_id())\n",
        ")\n",
        "\n",
        "display(narrow_df.orderBy(\"partition_id\", \"id\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how the `partition_id` values are usually preserved\n",
        "through narrow transformations (as long as we don't explicitly\n",
        "change the partitioning).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Example of a WIDE transformation: groupBy (causes shuffle)\n",
        "wide_df = (\n",
        "    df\n",
        "    .withColumn(\"key\", F.col(\"id\") % 3)\n",
        "    .groupBy(\"key\")\n",
        "    .agg(F.collect_list(\"id\").alias(\"ids\"))\n",
        ")\n",
        "\n",
        "wide_df.explain()  # Look for 'Exchange' in the plan (shuffle)\n",
        "display(wide_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the physical plan for `wide_df.explain()`, you should see an **Exchange**\n",
        "operator \u2013 this is Spark performing a shuffle for the `groupBy`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. `repartition` vs `coalesce` (DataFrame-only)\n",
        "\n",
        "- `repartition(n)`:\n",
        "  - Can **increase or decrease** partitions.\n",
        "  - Causes a **shuffle** (wide transformation).\n",
        "- `coalesce(n)`:\n",
        "  - Only **decreases** partitions.\n",
        "  - Tries to avoid a full shuffle (narrow-ish).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Start with our tiny df\n",
        "df_small = spark.range(0, 20).toDF(\"id\")\n",
        "\n",
        "# Repartition to 4 partitions (this will shuffle)\n",
        "df_repart = df_small.repartition(4)\n",
        "df_repart_pid = df_repart.withColumn(\"partition_id\", F.spark_partition_id())\n",
        "\n",
        "print(\"Number of partitions after repartition(4):\",\n",
        "      df_repart_pid.select(F.countDistinct(\"partition_id\")).first()[0])\n",
        "display(df_repart_pid.orderBy(\"partition_id\", \"id\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Coalesce down to 2 partitions (tries to avoid full shuffle)\n",
        "df_coal = df_repart.coalesce(2)\n",
        "df_coal_pid = df_coal.withColumn(\"partition_id\", F.spark_partition_id())\n",
        "\n",
        "print(\"Number of partitions after coalesce(2):\",\n",
        "      df_coal_pid.select(F.countDistinct(\"partition_id\")).first()[0])\n",
        "display(df_coal_pid.orderBy(\"partition_id\", \"id\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Real Dataset Example \u2013 `samples.nyctaxi.trips`\n",
        "\n",
        "Let's repeat some ideas on a real built-in dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "nyc_df = spark.read.table(\"samples.nyctaxi.trips\")\n",
        "\n",
        "nyc_with_pid = nyc_df.withColumn(\"partition_id\", F.spark_partition_id())\n",
        "\n",
        "# Show how many partitions and how many rows per partition (for the first few)\n",
        "partition_counts = (\n",
        "    nyc_with_pid\n",
        "    .groupBy(\"partition_id\")\n",
        "    .count()\n",
        "    .orderBy(\"partition_id\")\n",
        ")\n",
        "\n",
        "display(partition_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Narrow transformation: add column + filter\n",
        "nyc_narrow = (\n",
        "    nyc_df\n",
        "    .withColumn(\n",
        "        \"trip_duration_min\",\n",
        "        (F.col(\"tpep_dropoff_datetime\").cast(\"long\") -\n",
        "         F.col(\"tpep_pickup_datetime\").cast(\"long\")) / 60.0\n",
        "    )\n",
        "    .filter(F.col(\"trip_distance\") > 1.0)\n",
        "    .withColumn(\"partition_id\", F.spark_partition_id())\n",
        ")\n",
        "\n",
        "display(\n",
        "    nyc_narrow\n",
        "    .select(\"tpep_pickup_datetime\", \"trip_distance\", \"trip_duration_min\", \"partition_id\")\n",
        "    .orderBy(\"partition_id\", \"tpep_pickup_datetime\")\n",
        "    .limit(50)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Wide transformation: groupBy (shuffle)\n",
        "nyc_grouped = (\n",
        "    nyc_df\n",
        "    .groupBy(\"passenger_count\")\n",
        "    .agg(F.avg(\"trip_distance\").alias(\"avg_distance\"))\n",
        ")\n",
        "\n",
        "nyc_grouped.explain()  # Look for Exchange (shuffle)\n",
        "display(nyc_grouped.orderBy(\"passenger_count\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary for Students (DataFrame-only)\n",
        "\n",
        "- We can understand partitions using **`spark_partition_id()`** without touching RDDs.\n",
        "- Our \"glom\"-style view is just:\n",
        "  - add `partition_id`\n",
        "  - group by `partition_id`\n",
        "  - `collect_list` the rows.\n",
        "- **Narrow transforms** (e.g. `withColumn`, `filter`) usually keep partitioning.\n",
        "- **Wide transforms** (e.g. `groupBy`, `repartition`) cause shuffles.\n",
        "- `repartition(n)` \u2192 change to n partitions with shuffle.\n",
        "- `coalesce(n)` \u2192 reduce number of partitions with minimal shuffle.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}