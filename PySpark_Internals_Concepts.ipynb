{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6b73b4",
   "metadata": {},
   "source": [
    "# PySpark Internals and Concepts -- A Deep Dive\n",
    "\n",
    "## For Databricks Serverless Free Account\n",
    "\n",
    "This notebook explains the **conceptual foundations** of Apache Spark that every data\n",
    "engineer and analyst must understand. Instead of just showing syntax, we explain *why*\n",
    "things work the way they do and *how* Spark executes your code under the hood.\n",
    "\n",
    "All examples use inline sample data -- no external files or clusters needed.\n",
    "\n",
    "---\n",
    "\n",
    "### Topics Covered\n",
    "\n",
    "| # | Topic | Why It Matters |\n",
    "|---|---|---|\n",
    "| 1 | Lazy Evaluation | Spark does not run anything until you ask for a result |\n",
    "| 2 | Transformations vs Actions | The two categories every Spark operation falls into |\n",
    "| 3 | Narrow vs Wide Transformations | Determines whether Spark needs a shuffle |\n",
    "| 4 | Shuffle -- The Most Expensive Operation | Moving data across the network |\n",
    "| 5 | DAG, Jobs, Stages, Tasks | How Spark breaks your code into execution units |\n",
    "| 6 | Catalyst Optimizer and Tungsten | How Spark rewrites your query to make it faster |\n",
    "| 7 | Reading Explain Plans | The single most useful debugging skill |\n",
    "| 8 | Caching and Persistence | When to keep data in memory and when not to |\n",
    "| 9 | Broadcast Joins and Variables | Avoiding shuffle for small tables |\n",
    "| 10 | Partitioning (Memory and Disk) | Controlling how data is distributed |\n",
    "| 11 | Adaptive Query Execution (AQE) | Runtime optimizations (Databricks default) |\n",
    "| 12 | Data Skew and Salting | Fixing the most common performance problem |\n",
    "| 13 | Best Practices Checklist | Rules of thumb for production code |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1ccbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Setup: imports and sample data used throughout this notebook --\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# In Databricks, `spark` is pre-created. On local, uncomment:\n",
    "# spark = SparkSession.builder.appName(\"Concepts\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "print(f\"Spark version : {spark.version}\")\n",
    "print(f\"App name      : {spark.sparkContext.appName}\")\n",
    "\n",
    "# -- Sample data: 10 employees --\n",
    "data = [\n",
    "    (\"Alice\",   \"Engineering\", 95000,  30, \"2020-01-15\"),\n",
    "    (\"Bob\",     \"Marketing\",   72000,  28, \"2021-03-22\"),\n",
    "    (\"Charlie\", \"Engineering\", 110000, 35, \"2018-07-10\"),\n",
    "    (\"Diana\",   \"HR\",          68000,  26, \"2022-06-01\"),\n",
    "    (\"Eve\",     \"Marketing\",   85000,  32, \"2019-11-30\"),\n",
    "    (\"Frank\",   \"Engineering\", 102000, 40, \"2017-04-18\"),\n",
    "    (\"Grace\",   \"HR\",          71000,  29, \"2021-09-05\"),\n",
    "    (\"Hank\",    \"Sales\",       78000,  34, \"2020-02-14\"),\n",
    "    (\"Ivy\",     \"Sales\",       92000,  38, \"2018-12-20\"),\n",
    "    (\"Jack\",    \"Engineering\", 115000, 45, \"2016-08-25\"),\n",
    "]\n",
    "columns = [\"name\", \"department\", \"salary\", \"age\", \"hire_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1281fca",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Lazy Evaluation -- Spark Does Not Run Until You Ask\n",
    "\n",
    "### The Concept\n",
    "\n",
    "When you write `df.filter(...)` or `df.select(...)`, Spark does **nothing immediately**.\n",
    "It only records *what* you want to do -- it builds a logical plan. The actual computation\n",
    "happens only when you call an **action** like `.show()`, `.count()`, or `.collect()`.\n",
    "\n",
    "### Why?\n",
    "\n",
    "Because lazy evaluation allows Spark to **optimise the entire pipeline** before running it.\n",
    "If you filter 1 billion rows down to 100, and then select 2 columns out of 200, Spark\n",
    "can push the column selection *before* the filter, or skip reading columns it does not\n",
    "need. If it ran eagerly (line by line), it would miss these optimisation opportunities.\n",
    "\n",
    "### Analogy\n",
    "\n",
    "Think of it like a restaurant kitchen. The waiter (your code) writes down the full order\n",
    "(appetiser, main, dessert). The kitchen (Spark) does not start cooking the appetiser\n",
    "immediately -- it looks at the full order first and plans how to use the ovens and pans\n",
    "most efficiently. The order is \"lazy\"; the cooking starts only when the waiter says\n",
    "\"fire the order\" (an action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b826bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Demonstration of lazy evaluation --\n",
    "\n",
    "# These three lines add transformations to the plan.\n",
    "# Nothing is computed yet -- no data is scanned, no CPU time is spent.\n",
    "step1 = df.filter(F.col(\"salary\") > 80000)          # plan step 1\n",
    "step2 = step1.select(\"name\", \"department\", \"salary\") # plan step 2\n",
    "step3 = step2.withColumn(\"tax\", F.col(\"salary\") * 0.2)  # plan step 3\n",
    "\n",
    "# At this point 'step3' is just a plan (a recipe).\n",
    "# Prove it -- check the type:\n",
    "print(f\"Type of step3: {type(step3)}\")   # DataFrame, not a list of results\n",
    "\n",
    "# Now trigger an ACTION --> Spark compiles and runs the plan.\n",
    "step3.show()    # <-- THIS is when computation actually happens\n",
    "\n",
    "# Another action:\n",
    "print(f\"Row count: {step3.count()}\")     # <-- This runs the plan again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd6c2bb",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Transformations vs Actions -- The Two Categories\n",
    "\n",
    "Every Spark operation is either a **Transformation** or an **Action**.\n",
    "\n",
    "### Transformations (Lazy -- build the plan)\n",
    "\n",
    "A transformation takes a DataFrame and returns a **new** DataFrame. The original is never\n",
    "modified (DataFrames are immutable). Transformations are lazy -- they only add a step to\n",
    "the plan.\n",
    "\n",
    "| Transformation | What it does |\n",
    "|---|---|\n",
    "| `select()` | Pick columns |\n",
    "| `filter()` / `where()` | Keep rows matching a condition |\n",
    "| `withColumn()` | Add or replace a column |\n",
    "| `drop()` | Remove columns |\n",
    "| `groupBy()` | Group rows (must be followed by `agg()`) |\n",
    "| `join()` | Combine two DataFrames |\n",
    "| `orderBy()` / `sort()` | Sort rows |\n",
    "| `distinct()` | Remove duplicates |\n",
    "| `union()` / `unionAll()` | Stack two DataFrames vertically |\n",
    "| `repartition()` | Redistribute data across partitions |\n",
    "| `coalesce()` | Reduce partition count without full shuffle |\n",
    "\n",
    "### Actions (Eager -- trigger execution)\n",
    "\n",
    "An action forces Spark to execute all the queued transformations and return a result to\n",
    "the driver (your notebook) or write it to storage.\n",
    "\n",
    "| Action | What it returns |\n",
    "|---|---|\n",
    "| `show(n)` | Prints first n rows to console |\n",
    "| `count()` | Returns the number of rows (integer) |\n",
    "| `collect()` | Returns ALL rows as a Python list (careful with large data!) |\n",
    "| `first()` / `head()` | Returns the first row |\n",
    "| `take(n)` | Returns first n rows as a list |\n",
    "| `toPandas()` | Converts to a Pandas DataFrame (pulls all data to driver) |\n",
    "| `write.save()` | Writes data to storage (Delta, Parquet, CSV, etc.) |\n",
    "| `foreach()` | Applies a function to each row |\n",
    "| `describe()` | Returns summary statistics |\n",
    "\n",
    "### Key Rule\n",
    "\n",
    "**Each action triggers a separate job.** If you call `.show()` and then `.count()` on the\n",
    "same DataFrame, Spark runs the entire plan **twice** (unless you cache -- see Section 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Transformations vs Actions demo --\n",
    "\n",
    "# TRANSFORMATION chain (nothing runs yet)\n",
    "result = (\n",
    "    df\n",
    "    .filter(F.col(\"department\") == \"Engineering\")   # transformation\n",
    "    .select(\"name\", \"salary\")                       # transformation\n",
    "    .withColumn(\"bonus\", F.col(\"salary\") * 0.15)    # transformation\n",
    "    .orderBy(F.col(\"salary\").desc())                 # transformation\n",
    ")\n",
    "\n",
    "print(\"Plan built. No computation yet.\")\n",
    "\n",
    "# ACTION 1 -- triggers the full plan\n",
    "print(\"\\n--- Action: show() ---\")\n",
    "result.show()                                        # ACTION\n",
    "\n",
    "# ACTION 2 -- triggers the full plan AGAIN (from scratch)\n",
    "print(f\"--- Action: count() = {result.count()} ---\") # ACTION\n",
    "\n",
    "# ACTION 3 -- collect all rows to driver as a Python list\n",
    "rows = result.collect()                               # ACTION\n",
    "print(f\"--- Action: collect() returned {len(rows)} Python Row objects ---\")\n",
    "print(f\"First row: {rows[0]}\")\n",
    "\n",
    "# ACTION 4 -- convert to Pandas (pulls everything to driver memory)\n",
    "pdf = result.toPandas()                               # ACTION\n",
    "print(f\"\\n--- Action: toPandas() ---\")\n",
    "print(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee6f9d",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Narrow vs Wide Transformations\n",
    "\n",
    "Not all transformations are equal. Spark classifies them by how much data needs to\n",
    "move between machines.\n",
    "\n",
    "### Narrow Transformations (No Shuffle)\n",
    "\n",
    "Each output partition depends on **exactly one** input partition. Data stays on the same\n",
    "machine. These are fast.\n",
    "\n",
    "```\n",
    "Partition 1 --> Partition 1\n",
    "Partition 2 --> Partition 2\n",
    "Partition 3 --> Partition 3\n",
    "```\n",
    "\n",
    "**Examples:** `select`, `filter`, `withColumn`, `map`, `flatMap`, `union`, `coalesce`\n",
    "\n",
    "### Wide Transformations (Requires Shuffle)\n",
    "\n",
    "Each output partition may depend on **multiple** input partitions. Data must be moved\n",
    "across the network (shuffled). These are expensive.\n",
    "\n",
    "```\n",
    "Partition 1 ──┐\n",
    "Partition 2 ──┼──> Partition A\n",
    "Partition 3 ──┘\n",
    "```\n",
    "\n",
    "**Examples:** `groupBy`, `join`, `orderBy/sort`, `distinct`, `repartition`,\n",
    "`reduceByKey`, `aggregateByKey`\n",
    "\n",
    "### Why Does This Matter?\n",
    "\n",
    "- A **shuffle** means Spark must write intermediate data to disk, send it over the\n",
    "  network, and read it back. On large datasets this can take minutes.\n",
    "- Spark creates a **new stage** at every shuffle boundary. More shuffles = more stages\n",
    "  = more overhead.\n",
    "- Reducing unnecessary shuffles is the single biggest performance optimisation you can\n",
    "  make.\n",
    "\n",
    "### Visual: Narrow vs Wide\n",
    "\n",
    "```\n",
    "NARROW (fast):                    WIDE (slow):\n",
    "+--------+    +--------+         +--------+\n",
    "| Part 1 | -> | Part 1 |         | Part 1 |---\\\n",
    "+--------+    +--------+         +--------+    \\   +--------+\n",
    "+--------+    +--------+         +--------+     +->| Part A |\n",
    "| Part 2 | -> | Part 2 |         | Part 2 |---/   +--------+\n",
    "+--------+    +--------+         +--------+\n",
    "                                  Each output needs data from ALL inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51071045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Narrow vs Wide -- see it in the execution plan --\n",
    "\n",
    "# NARROW transformations only -- no shuffle.\n",
    "narrow_result = df.filter(F.col(\"salary\") > 80000).select(\"name\", \"salary\")\n",
    "print(\"=== NARROW plan (no Exchange node) ===\")\n",
    "narrow_result.explain()\n",
    "# You will NOT see \"Exchange\" in the plan.\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "# WIDE transformation -- groupBy triggers a shuffle.\n",
    "wide_result = df.groupBy(\"department\").agg(F.avg(\"salary\").alias(\"avg_salary\"))\n",
    "print(\"=== WIDE plan (Exchange = shuffle) ===\")\n",
    "wide_result.explain()\n",
    "# You WILL see \"Exchange hashpartitioning\" -- that is the shuffle.\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "# WIDE -- orderBy also triggers a shuffle (global sort needs all data).\n",
    "sorted_result = df.orderBy(F.col(\"salary\").desc())\n",
    "print(\"=== WIDE plan (orderBy = shuffle) ===\")\n",
    "sorted_result.explain()\n",
    "# \"Exchange rangepartitioning\" = data is shuffled for the sort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8599cb5f",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Shuffle -- The Most Expensive Operation in Spark\n",
    "\n",
    "### What Happens During a Shuffle\n",
    "\n",
    "1. **Map side (write):** Each executor writes its portion of data to local disk,\n",
    "   organised by the target partition key (hash or range).\n",
    "2. **Network transfer:** Data is sent from every mapper to every reducer over the\n",
    "   network.\n",
    "3. **Reduce side (read):** Each reducer reads the pieces it needs from all mappers and\n",
    "   combines them.\n",
    "\n",
    "### Shuffle is expensive because:\n",
    "\n",
    "- **Disk I/O:** Intermediate data is serialised and written to disk.\n",
    "- **Network I/O:** Data crosses the network; bandwidth is limited.\n",
    "- **Serialisation:** Data is converted from internal format to bytes and back.\n",
    "- **Memory pressure:** Buffers fill up, causing spills to disk.\n",
    "\n",
    "### When does Shuffle happen?\n",
    "\n",
    "| Operation | Shuffle? | Why |\n",
    "|---|---|---|\n",
    "| `filter`, `select`, `withColumn` | No | Each row is independent |\n",
    "| `groupBy().agg()` | Yes | Rows with the same key must be on the same machine |\n",
    "| `join` (sort-merge) | Yes | Matching keys need to be co-located |\n",
    "| `join` (broadcast) | No | Small table is copied to all nodes |\n",
    "| `orderBy` / `sort` | Yes | Global ordering requires all data in one place |\n",
    "| `distinct` | Yes | Must compare all rows |\n",
    "| `repartition(n)` | Yes | Explicitly redistributes data |\n",
    "| `coalesce(n)` | No | Only merges existing partitions (no data movement) |\n",
    "\n",
    "### How to Reduce Shuffle\n",
    "\n",
    "1. **Filter early** -- reduce data volume before a groupBy or join.\n",
    "2. **Broadcast small tables** -- avoids shuffle in joins.\n",
    "3. **Use coalesce instead of repartition** when reducing partition count.\n",
    "4. **Pre-partition data on join/group keys** when writing to storage.\n",
    "5. **Avoid unnecessary sorts** -- use `orderBy` only when the consumer needs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Seeing shuffle in action --\n",
    "\n",
    "# Query 1: groupBy causes a shuffle\n",
    "grouped = df.groupBy(\"department\").agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.round(F.avg(\"salary\"), 2).alias(\"avg_salary\")\n",
    ")\n",
    "print(\"=== groupBy plan -- look for 'Exchange' ===\")\n",
    "grouped.explain()\n",
    "grouped.show()\n",
    "\n",
    "# Query 2: join causes a shuffle (sort-merge join by default)\n",
    "dept_data = [(\"Engineering\", \"Building A\"), (\"Marketing\", \"Building B\"),\n",
    "             (\"HR\", \"Building C\"), (\"Sales\", \"Building D\")]\n",
    "df_dept = spark.createDataFrame(dept_data, [\"department\", \"location\"])\n",
    "\n",
    "joined = df.join(df_dept, on=\"department\", how=\"inner\")\n",
    "print(\"\\n=== join plan -- TWO Exchanges (one per side) ===\")\n",
    "joined.explain()\n",
    "\n",
    "# Query 3: broadcast join -- NO shuffle for the small table\n",
    "joined_broadcast = df.join(F.broadcast(df_dept), on=\"department\", how=\"inner\")\n",
    "print(\"\\n=== broadcast join plan -- only BroadcastHashJoin, no Exchange ===\")\n",
    "joined_broadcast.explain()\n",
    "joined_broadcast.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f46303",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. DAG, Jobs, Stages, and Tasks -- How Spark Executes Your Code\n",
    "\n",
    "### The Execution Hierarchy\n",
    "\n",
    "```\n",
    "YOUR CODE\n",
    "   |\n",
    "   v\n",
    " Logical Plan  (what you want)\n",
    "   |\n",
    "   v  [Catalyst Optimizer]\n",
    " Optimised Logical Plan\n",
    "   |\n",
    "   v\n",
    " Physical Plan (how to do it)\n",
    "   |\n",
    "   v\n",
    " JOB  (triggered by each action: .show(), .count(), .write)\n",
    "   |\n",
    "   +-- STAGE 1  (runs until a shuffle boundary)\n",
    "   |     +-- Task 1.1  (one per partition)\n",
    "   |     +-- Task 1.2\n",
    "   |     +-- Task 1.3\n",
    "   |\n",
    "   +-- STAGE 2  (starts after shuffle from Stage 1)\n",
    "         +-- Task 2.1\n",
    "         +-- Task 2.2\n",
    "```\n",
    "\n",
    "### Definitions\n",
    "\n",
    "| Term | What it is |\n",
    "|---|---|\n",
    "| **DAG** (Directed Acyclic Graph) | The full graph of transformations from source to action. Spark builds this from your code. |\n",
    "| **Job** | One unit of work triggered by an action. Each `.show()`, `.count()`, or `.write()` creates one job. |\n",
    "| **Stage** | A subset of the job that can run without a shuffle. Stages are separated by shuffle (Exchange) boundaries. |\n",
    "| **Task** | The smallest unit of work. One task processes one partition in one stage. If a stage has 200 partitions, it has 200 tasks. |\n",
    "\n",
    "### How to see Jobs and Stages\n",
    "\n",
    "In Databricks, open the **Spark UI** (link at the top of any cell output). You will see:\n",
    "- **Jobs tab:** One row per action.\n",
    "- **Stages tab:** Shows how many stages each job has and where the shuffle boundaries are.\n",
    "- **Tasks:** Inside each stage, you can see how many tasks ran, their duration, and\n",
    "  whether any were slow (skewed).\n",
    "\n",
    "### Practical Impact\n",
    "\n",
    "- **Too few partitions (e.g., 1):** Only 1 task runs -- no parallelism.\n",
    "- **Too many partitions (e.g., 10,000 for 100 rows):** Excessive overhead scheduling\n",
    "  10,000 tiny tasks.\n",
    "- **Rule of thumb:** 2-4 partitions per CPU core, each partition 100-200 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Inspecting DAG, partitions, and stages --\n",
    "\n",
    "# How many partitions does our DataFrame have?\n",
    "print(f\"Partitions in df: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# A query with TWO stages (one shuffle from groupBy):\n",
    "two_stage_query = (\n",
    "    df\n",
    "    .filter(F.col(\"salary\") > 70000)                          # Stage 1: narrow\n",
    "    .groupBy(\"department\")                                     # shuffle boundary\n",
    "    .agg(F.sum(\"salary\").alias(\"total\"), F.count(\"*\").alias(\"n\"))  # Stage 2: aggregate\n",
    ")\n",
    "\n",
    "print(\"=== Physical plan -- each Exchange = new stage ===\")\n",
    "two_stage_query.explain(mode=\"formatted\")\n",
    "\n",
    "# A query with THREE stages (two shuffles: groupBy + orderBy):\n",
    "three_stage_query = (\n",
    "    df\n",
    "    .groupBy(\"department\")                                     # shuffle 1\n",
    "    .agg(F.avg(\"salary\").alias(\"avg_sal\"))\n",
    "    .orderBy(F.col(\"avg_sal\").desc())                          # shuffle 2\n",
    ")\n",
    "\n",
    "print(\"\\n=== Three-stage plan ===\")\n",
    "three_stage_query.explain(mode=\"formatted\")\n",
    "three_stage_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfe36de",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Catalyst Optimizer and Tungsten Engine\n",
    "\n",
    "### Catalyst Optimizer -- Rewrites Your Query\n",
    "\n",
    "When you write PySpark code, it goes through four stages of optimisation:\n",
    "\n",
    "```\n",
    "1. PARSED Logical Plan     -- raw translation of your code\n",
    "       |\n",
    "2. ANALYZED Logical Plan   -- columns and types resolved against the catalog\n",
    "       |\n",
    "3. OPTIMIZED Logical Plan  -- rules applied to make it faster\n",
    "       |\n",
    "4. PHYSICAL Plan           -- the actual execution strategy chosen\n",
    "```\n",
    "\n",
    "### Common Catalyst Optimisation Rules\n",
    "\n",
    "| Rule | What it does | Example |\n",
    "|---|---|---|\n",
    "| **Predicate Pushdown** | Moves filters as close to the data source as possible | If you filter after a join, Catalyst pushes the filter *before* the join to reduce data early |\n",
    "| **Column Pruning** | Reads only the columns you actually use | You select 3 columns from a 200-column Parquet file -- Spark skips the other 197 |\n",
    "| **Constant Folding** | Pre-computes constant expressions | `F.lit(2) * F.lit(3)` becomes `F.lit(6)` at plan time |\n",
    "| **Join Reordering** | Reorders joins to minimise intermediate data | Puts the smaller table on the build side of a hash join |\n",
    "| **Broadcast Detection** | Auto-broadcasts small tables | If a table is below `spark.sql.autoBroadcastJoinThreshold` (default 10 MB), Spark broadcasts it |\n",
    "\n",
    "### Tungsten Engine -- How Data Is Stored in Memory\n",
    "\n",
    "Tungsten manages memory at the byte level instead of using Java objects. This means:\n",
    "- **Less garbage collection** -- fewer Java objects to track.\n",
    "- **Cache-friendly layouts** -- data is stored in contiguous memory for CPU efficiency.\n",
    "- **Code generation** -- Spark generates Java bytecode at runtime for your specific\n",
    "  query, avoiding the overhead of generic interpreters.\n",
    "\n",
    "You do not need to configure Tungsten -- it is always on. But knowing it exists helps\n",
    "you understand why PySpark is faster than naive Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8016730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- See Catalyst in action: Predicate Pushdown --\n",
    "\n",
    "# Write a query where filter comes AFTER the join.\n",
    "# Catalyst will push the filter BEFORE the join automatically.\n",
    "\n",
    "result = (\n",
    "    df.join(df_dept, on=\"department\")\n",
    "    .filter(F.col(\"salary\") > 90000)          # written after join\n",
    "    .select(\"name\", \"department\", \"salary\", \"location\")\n",
    ")\n",
    "\n",
    "print(\"=== Extended plan -- watch 'PushedFilters' and filter position ===\")\n",
    "result.explain(mode=\"extended\")\n",
    "# In the optimised plan, you will see the Filter node moved below the Join.\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# -- Column pruning --\n",
    "# We only select 2 columns. Spark will not read the rest from a Parquet/Delta source.\n",
    "pruned = df.select(\"name\", \"salary\")\n",
    "print(\"\\n=== Column Pruning ===\")\n",
    "pruned.explain()\n",
    "# On a file-based source you would see: ReadSchema with only name and salary.\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# -- Compare all four plan stages --\n",
    "print(\"\\n=== All four plan stages (parsed -> analyzed -> optimized -> physical) ===\")\n",
    "(\n",
    "    df\n",
    "    .filter(F.col(\"department\") == \"Engineering\")\n",
    "    .groupBy(\"department\")\n",
    "    .agg(F.avg(\"salary\").alias(\"avg_sal\"))\n",
    ").explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e321f182",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Reading Explain Plans -- The Most Useful Debugging Skill\n",
    "\n",
    "### How to Read an Explain Plan\n",
    "\n",
    "Plans are printed **bottom to top** -- the bottom is where Spark starts (reading data),\n",
    "and the top is where Spark finishes (returning results).\n",
    "\n",
    "### Key Nodes to Look For\n",
    "\n",
    "| Node Name | What It Means |\n",
    "|---|---|\n",
    "| `Scan` / `FileScan` | Reading data from a source (Delta, Parquet, CSV, memory) |\n",
    "| `Filter` | Applying a WHERE condition |\n",
    "| `Project` | Selecting or computing columns |\n",
    "| `Exchange` | **SHUFFLE** -- data is redistributed across partitions. This is the expensive one. |\n",
    "| `HashAggregate` | Computing aggregations (sum, avg, count, etc.) |\n",
    "| `SortMergeJoin` | Joining two large tables (both sides shuffled and sorted) |\n",
    "| `BroadcastHashJoin` | Joining with a small table broadcast to all nodes (fast, no shuffle) |\n",
    "| `BroadcastExchange` | Sending the small table to all executors |\n",
    "| `Sort` | Sorting data |\n",
    "| `WholeStageCodegen` | Tungsten has generated optimised bytecode for this section |\n",
    "\n",
    "### explain() Modes\n",
    "\n",
    "| Mode | What it shows |\n",
    "|---|---|\n",
    "| `explain()` or `explain(False)` | Physical plan only (most common) |\n",
    "| `explain(True)` or `explain(mode=\"extended\")` | All four plans: parsed, analyzed, optimised, physical |\n",
    "| `explain(mode=\"formatted\")` | Nicely formatted physical plan (best for Databricks) |\n",
    "| `explain(mode=\"cost\")` | Plan with estimated row counts and sizes |\n",
    "| `explain(mode=\"codegen\")` | The generated Java code (advanced) |\n",
    "\n",
    "### What to Look For When Debugging Slow Queries\n",
    "\n",
    "1. **How many Exchange nodes?** Each one is a shuffle. Can you eliminate any?\n",
    "2. **SortMergeJoin vs BroadcastHashJoin?** If one side is small, broadcast it.\n",
    "3. **Filter position:** Is it before or after the join? It should be before.\n",
    "4. **Scan:** How many columns are being read? Are partition filters being applied?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c680919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Reading explain plans: a guided example --\n",
    "\n",
    "# A realistic query: \"Average salary by department for employees earning > 70K,\n",
    "# enriched with department location.\"\n",
    "query = (\n",
    "    df\n",
    "    .filter(F.col(\"salary\") > 70000)                           # filter early\n",
    "    .join(F.broadcast(df_dept), on=\"department\", how=\"inner\")   # broadcast the small table\n",
    "    .groupBy(\"department\", \"location\")\n",
    "    .agg(\n",
    "        F.round(F.avg(\"salary\"), 2).alias(\"avg_salary\"),\n",
    "        F.count(\"*\").alias(\"emp_count\")\n",
    "    )\n",
    "    .orderBy(F.col(\"avg_salary\").desc())\n",
    ")\n",
    "\n",
    "print(\"=== FORMATTED plan -- read bottom to top ===\")\n",
    "query.explain(mode=\"formatted\")\n",
    "\n",
    "# What you should see (bottom to top):\n",
    "# 1. Scan (read df from memory)\n",
    "# 2. Filter (salary > 70000)           <-- Catalyst pushed this before the join\n",
    "# 3. BroadcastExchange (send df_dept to all nodes)\n",
    "# 4. BroadcastHashJoin                 <-- no shuffle for the join!\n",
    "# 5. HashAggregate (partial)           <-- pre-aggregation on each partition\n",
    "# 6. Exchange hashpartitioning         <-- shuffle to group by department\n",
    "# 7. HashAggregate (final)             <-- final aggregation\n",
    "# 8. Exchange rangepartitioning        <-- shuffle for orderBy\n",
    "# 9. Sort                              <-- final ordering\n",
    "\n",
    "print(\"\\n=== RESULT ===\")\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e0d766",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Caching and Persistence -- When to Keep Data in Memory\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Every time you call an action (`.show()`, `.count()`, `.write()`), Spark re-executes the\n",
    "entire plan from scratch -- reading data, applying filters, joins, etc. If you use the\n",
    "same transformed DataFrame in 5 different places, Spark does the work **5 times**.\n",
    "\n",
    "### The Solution: cache() and persist()\n",
    "\n",
    "Calling `.cache()` tells Spark: \"After computing this DataFrame for the first time, keep\n",
    "the result in memory so future actions can reuse it instantly.\"\n",
    "\n",
    "### cache() vs persist()\n",
    "\n",
    "| Method | Storage Level | When to Use |\n",
    "|---|---|---|\n",
    "| `df.cache()` | `MEMORY_AND_DISK` | Default choice. Stores in memory; spills to disk if memory is full. |\n",
    "| `df.persist(StorageLevel.MEMORY_ONLY)` | Memory only | When you have enough memory and want maximum speed. If it does not fit, partitions are recomputed. |\n",
    "| `df.persist(StorageLevel.DISK_ONLY)` | Disk only | When data is too large for memory but you still want to avoid recomputation. |\n",
    "| `df.persist(StorageLevel.MEMORY_AND_DISK)` | Same as cache() | Explicit equivalent of `.cache()`. |\n",
    "| `df.persist(StorageLevel.MEMORY_AND_DISK_SER)` | Serialised | Uses less memory (compressed) but slower to read back. |\n",
    "\n",
    "### When to Cache\n",
    "\n",
    "- You use the same DataFrame in **multiple actions** (e.g., show + count + write).\n",
    "- The DataFrame involves **expensive transformations** (joins, aggregations, UDFs).\n",
    "- The DataFrame is used in a **loop** (e.g., iterative ML algorithms).\n",
    "\n",
    "### When NOT to Cache\n",
    "\n",
    "- The DataFrame is used **only once** -- caching adds overhead for no benefit.\n",
    "- The data is **too large** to fit in memory -- you get spills and slowdowns.\n",
    "- You are reading from a **Delta table with aggressive caching in the storage layer**\n",
    "  (Databricks already caches Delta files via Delta Cache).\n",
    "\n",
    "### Important: Cache is Lazy\n",
    "\n",
    "`.cache()` itself does not trigger computation. The data is only cached when the **first\n",
    "action** runs. After that, subsequent actions reuse the cached data.\n",
    "\n",
    "### Cleanup with unpersist()\n",
    "\n",
    "Always call `df.unpersist()` when you are done with a cached DataFrame. Cached data\n",
    "occupies cluster memory that other queries could use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb669c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# -- WITHOUT caching: plan runs twice --\n",
    "expensive_df = (\n",
    "    df\n",
    "    .filter(F.col(\"salary\") > 70000)\n",
    "    .join(F.broadcast(df_dept), on=\"department\")\n",
    "    .withColumn(\"bonus\", F.col(\"salary\") * 0.1)\n",
    ")\n",
    "\n",
    "print(\"=== Without cache: each action recomputes ===\")\n",
    "expensive_df.show(3)       # computes the full plan\n",
    "print(f\"Count: {expensive_df.count()}\")  # computes the full plan AGAIN\n",
    "\n",
    "# -- WITH caching: plan runs once, result is reused --\n",
    "expensive_df.cache()       # mark for caching (lazy -- nothing happens yet)\n",
    "expensive_df.count()       # first action: computes AND caches\n",
    "print(f\"Is cached: {expensive_df.is_cached}\")\n",
    "\n",
    "print(\"\\n=== With cache: subsequent actions use cached data ===\")\n",
    "expensive_df.show(3)                                 # instant -- from cache\n",
    "print(f\"Count: {expensive_df.count()}\")              # instant -- from cache\n",
    "expensive_df.groupBy(\"department\").count().show()     # instant -- from cache\n",
    "\n",
    "# Check the explain plan -- you will see \"InMemoryTableScan\" instead of the full plan\n",
    "print(\"=== Plan after caching (InMemoryTableScan) ===\")\n",
    "expensive_df.explain()\n",
    "\n",
    "# -- Cleanup --\n",
    "expensive_df.unpersist()\n",
    "print(f\"\\nAfter unpersist, is cached: {expensive_df.is_cached}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- persist() with different storage levels --\n",
    "\n",
    "# MEMORY_ONLY -- fastest, but if data does not fit, partitions are recomputed\n",
    "df_mem = df.persist(StorageLevel.MEMORY_ONLY)\n",
    "df_mem.count()\n",
    "print(f\"MEMORY_ONLY cached: {df_mem.is_cached}\")\n",
    "df_mem.unpersist()\n",
    "\n",
    "# DISK_ONLY -- slowest, but useful for very large intermediate results\n",
    "df_disk = df.persist(StorageLevel.DISK_ONLY)\n",
    "df_disk.count()\n",
    "print(f\"DISK_ONLY cached:   {df_disk.is_cached}\")\n",
    "df_disk.unpersist()\n",
    "\n",
    "# MEMORY_AND_DISK_SER -- serialised (compressed), uses less memory, slower to read\n",
    "df_ser = df.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "df_ser.count()\n",
    "print(f\"MEMORY_AND_DISK_SER cached: {df_ser.is_cached}\")\n",
    "df_ser.unpersist()\n",
    "\n",
    "# -- Check what is currently cached in the cluster --\n",
    "print(\"\\nStorage info:\")\n",
    "for rdd_info in spark.sparkContext._jsc.sc().getRDDStorageInfo():\n",
    "    print(f\"  {rdd_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713d6dc",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Broadcast Joins and Broadcast Variables\n",
    "\n",
    "### The Problem with Regular Joins\n",
    "\n",
    "A regular join (SortMergeJoin) shuffles **both** tables so that rows with the same key\n",
    "land on the same partition. If one table has 1 billion rows and the other has 100 rows,\n",
    "shuffling the 100-row table is wasteful.\n",
    "\n",
    "### Broadcast Join -- The Solution for Small Tables\n",
    "\n",
    "Instead of shuffling, Spark copies (broadcasts) the small table to every executor's\n",
    "memory. Each executor then joins its partition of the large table with the local copy\n",
    "of the small table. **No shuffle on either side.**\n",
    "\n",
    "```\n",
    "SORT-MERGE JOIN (both shuffle):         BROADCAST JOIN (no shuffle):\n",
    "Large Table  -->  shuffle  --|          Large Table  (stays in place)\n",
    "                              +-join        +\n",
    "Small Table  -->  shuffle  --|          Small Table  (copied to all nodes)\n",
    "```\n",
    "\n",
    "### When Does Spark Auto-Broadcast?\n",
    "\n",
    "Spark broadcasts automatically when a table is smaller than\n",
    "`spark.sql.autoBroadcastJoinThreshold` (default: **10 MB**).\n",
    "\n",
    "You can also force it manually with `F.broadcast(df)`.\n",
    "\n",
    "### Broadcast Variables (General Purpose)\n",
    "\n",
    "A **broadcast variable** is not limited to joins. You can broadcast any Python object\n",
    "(dictionary, list, model) to all executors for use inside UDFs or transformations.\n",
    "\n",
    "```python\n",
    "lookup = spark.sparkContext.broadcast({\"key\": \"value\"})\n",
    "# Access in UDF: lookup.value[\"key\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43250b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Broadcast join vs sort-merge join comparison --\n",
    "\n",
    "# Check the auto-broadcast threshold\n",
    "threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "print(f\"Auto broadcast threshold: {threshold} bytes ({int(threshold) / 1024 / 1024:.0f} MB)\")\n",
    "\n",
    "# SORT-MERGE JOIN -- disable auto-broadcast to force it\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)  # disable auto-broadcast\n",
    "\n",
    "smj = df.join(df_dept, on=\"department\")\n",
    "print(\"=== Sort-Merge Join (both sides shuffled) ===\")\n",
    "smj.explain()\n",
    "# You will see: Exchange hashpartitioning on BOTH sides + SortMergeJoin\n",
    "\n",
    "# BROADCAST JOIN -- explicitly broadcast the small table\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)  # restore default (10 MB)\n",
    "\n",
    "bhj = df.join(F.broadcast(df_dept), on=\"department\")\n",
    "print(\"\\n=== Broadcast Hash Join (no shuffle) ===\")\n",
    "bhj.explain()\n",
    "# You will see: BroadcastExchange + BroadcastHashJoin , NO Exchange on df side\n",
    "\n",
    "bhj.show()\n",
    "\n",
    "# -- Broadcast variable example (for UDFs) --\n",
    "# A lookup dictionary broadcast to all executors.\n",
    "dept_codes = {\"Engineering\": \"ENG\", \"Marketing\": \"MKT\", \"HR\": \"HR\", \"Sales\": \"SAL\"}\n",
    "broadcast_codes = spark.sparkContext.broadcast(dept_codes)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(StringType())\n",
    "def get_dept_code(dept):\n",
    "    return broadcast_codes.value.get(dept, \"UNK\")\n",
    "\n",
    "df.select(\"name\", \"department\", get_dept_code(\"department\").alias(\"dept_code\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822fd51",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Partitioning -- Controlling Data Distribution\n",
    "\n",
    "### What is a Partition?\n",
    "\n",
    "A partition is a chunk of data that one task processes. Spark splits your DataFrame into\n",
    "multiple partitions and processes them in parallel across the cluster.\n",
    "\n",
    "### In-Memory Partitioning (repartition vs coalesce)\n",
    "\n",
    "| Method | What it does | Shuffle? |\n",
    "|---|---|---|\n",
    "| `repartition(n)` | Redistributes data into exactly `n` partitions | Yes (full shuffle) |\n",
    "| `repartition(\"col\")` | Redistributes by column hash (same key values go to same partition) | Yes |\n",
    "| `coalesce(n)` | Reduces partitions by merging (only reduces, cannot increase) | No (just merges) |\n",
    "\n",
    "### When to Repartition\n",
    "\n",
    "- **Before a join or groupBy on a specific column:** Pre-partitioning on the join key\n",
    "  avoids an extra shuffle during the join.\n",
    "- **Before writing to storage:** Control the number of output files.\n",
    "- **When partitions are very uneven (skewed).**\n",
    "\n",
    "### On-Disk Partitioning (partitionBy in write)\n",
    "\n",
    "When you write data to Delta/Parquet with `.partitionBy(\"column\")`, Spark creates separate\n",
    "folders for each unique value. When you later read with a filter on that column, Spark\n",
    "skips entire folders -- this is called **partition pruning**.\n",
    "\n",
    "```python\n",
    "df.write.format(\"delta\").partitionBy(\"department\").save(\"/path\")\n",
    "# Creates folders: department=Engineering/, department=HR/, etc.\n",
    "# Reading with filter(department == 'HR') only scans the HR folder.\n",
    "```\n",
    "\n",
    "### Choosing Partition Columns\n",
    "\n",
    "- Pick a column you **frequently filter on** (date, region, department).\n",
    "- The column should have **moderate cardinality** (10-1000 unique values).\n",
    "  - Too few (2-3): Large partitions, no benefit.\n",
    "  - Too many (millions): Millions of tiny files -- the \"small files problem\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55790f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Partitioning in practice --\n",
    "\n",
    "# Check current partitions\n",
    "print(f\"Default partitions in df: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# repartition(n) -- redistribute into n partitions (FULL SHUFFLE)\n",
    "df_4 = df.repartition(4)\n",
    "print(f\"After repartition(4): {df_4.rdd.getNumPartitions()}\")\n",
    "\n",
    "# repartition by column -- same department values go to the same partition\n",
    "df_by_dept = df.repartition(\"department\")\n",
    "print(f\"After repartition('department'): {df_by_dept.rdd.getNumPartitions()}\")\n",
    "\n",
    "# See how data is distributed across partitions\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "df_by_dept.withColumn(\"partition_id\", spark_partition_id()).groupBy(\"partition_id\", \"department\").count().orderBy(\"partition_id\").show()\n",
    "\n",
    "# coalesce(n) -- reduce partitions WITHOUT a shuffle (only merges)\n",
    "df_2 = df_4.coalesce(2)\n",
    "print(f\"After coalesce(2): {df_2.rdd.getNumPartitions()}\")\n",
    "\n",
    "# -- On-disk partitioning --\n",
    "delta_path = \"/tmp/concepts_tutorial/emp_partitioned\"\n",
    "df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"department\").save(delta_path)\n",
    "\n",
    "# When you filter by department, Spark only reads that folder (partition pruning)\n",
    "print(\"\\n=== Partition pruning -- only scans Engineering folder ===\")\n",
    "spark.read.format(\"delta\").load(delta_path).filter(F.col(\"department\") == \"Engineering\").explain()\n",
    "\n",
    "# Cleanup\n",
    "dbutils.fs.rm(\"/tmp/concepts_tutorial\", recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2400abe",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Adaptive Query Execution (AQE) -- Runtime Optimisation\n",
    "\n",
    "### What is AQE?\n",
    "\n",
    "AQE is a runtime optimisation framework that was introduced in Spark 3.0 and is\n",
    "**enabled by default in Databricks**. Unlike Catalyst (which optimises at plan time),\n",
    "AQE optimises **during execution** based on real statistics.\n",
    "\n",
    "### What AQE Does\n",
    "\n",
    "| Feature | What it does | Why it helps |\n",
    "|---|---|---|\n",
    "| **Coalescing shuffle partitions** | After a shuffle, AQE merges tiny partitions into larger ones | Reduces task scheduling overhead |\n",
    "| **Converting joins** | Converts SortMergeJoin to BroadcastHashJoin at runtime if one side is small enough after filtering | Eliminates a shuffle mid-execution |\n",
    "| **Skew join optimisation** | Detects partitions that are much larger than others and splits them | Prevents one task from running much longer than the rest |\n",
    "\n",
    "### Example: Dynamic Partition Coalescing\n",
    "\n",
    "Without AQE, the default shuffle partition count (`spark.sql.shuffle.partitions`) is 200.\n",
    "If your data produces only 10 groups, 190 partitions are empty -- wasted overhead.\n",
    "AQE detects this and merges the empty partitions into the 10 useful ones.\n",
    "\n",
    "### Configuration\n",
    "\n",
    "```python\n",
    "# Check if AQE is enabled (it should be on Databricks)\n",
    "spark.conf.get(\"spark.sql.adaptive.enabled\")  # \"true\"\n",
    "\n",
    "# Key AQE settings\n",
    "spark.conf.get(\"spark.sql.adaptive.coalescePartitions.enabled\")      # true\n",
    "spark.conf.get(\"spark.sql.adaptive.skewJoin.enabled\")                # true\n",
    "spark.conf.get(\"spark.sql.adaptive.localShuffleReader.enabled\")      # true\n",
    "```\n",
    "\n",
    "### Should You Do Anything?\n",
    "\n",
    "On Databricks Serverless, AQE is fully managed. You rarely need to tune it. The main\n",
    "scenario where you still intervene is **severe data skew** that AQE cannot fully resolve\n",
    "-- in that case, use manual salting (Section 12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c877ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- AQE configuration check --\n",
    "\n",
    "print(\"=== Adaptive Query Execution Settings ===\")\n",
    "aqe_settings = [\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
    "    \"spark.sql.adaptive.skewJoin.enabled\",\n",
    "    \"spark.sql.adaptive.localShuffleReader.enabled\",\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\",\n",
    "]\n",
    "\n",
    "for setting in aqe_settings:\n",
    "    try:\n",
    "        print(f\"  {setting} = {spark.conf.get(setting)}\")\n",
    "    except Exception:\n",
    "        print(f\"  {setting} = (not set)\")\n",
    "\n",
    "# -- Demonstrate AQE coalescing partitions --\n",
    "# Default shuffle.partitions is 200. With only 4 departments, 196 partitions are empty.\n",
    "# AQE will coalesce them automatically.\n",
    "\n",
    "print(f\"\\nDefault shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "grouped = df.groupBy(\"department\").agg(F.sum(\"salary\").alias(\"total\"))\n",
    "grouped.show()\n",
    "\n",
    "# In the Spark UI, you will see that AQE reduced the actual partition count\n",
    "# from 200 to something much smaller (typically 1-4 for this small dataset).\n",
    "print(\"Check the Spark UI -> SQL tab to see AQE's CustomShuffleReader coalescing partitions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e2280",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Data Skew and Salting -- Fixing the Most Common Performance Problem\n",
    "\n",
    "### What is Data Skew?\n",
    "\n",
    "Data skew means one partition has **much more data** than others. When you do a\n",
    "`groupBy(\"customer_id\")` or `join` on a skewed key, one task gets millions of rows while\n",
    "others get thousands. The slow task becomes a bottleneck -- all other tasks finish quickly\n",
    "but the job waits for the one slow task.\n",
    "\n",
    "### How to Detect Skew\n",
    "\n",
    "1. **Spark UI -> Stages -> Summary Metrics:** Look at the median vs max task duration.\n",
    "   If the max is 10x+ the median, you have skew.\n",
    "2. **Check value distribution:** `df.groupBy(\"key\").count().orderBy(F.desc(\"count\")).show()`\n",
    "\n",
    "### Salting -- The Manual Fix\n",
    "\n",
    "Salting adds a random number to the skewed key, spreading its rows across multiple\n",
    "partitions. After the aggregation or join, you remove the salt.\n",
    "\n",
    "```\n",
    "Before salting:           After salting (salt 0-3):\n",
    "Key \"USA\" -> 1M rows      Key \"USA_0\" -> 250K rows\n",
    "                           Key \"USA_1\" -> 250K rows\n",
    "                           Key \"USA_2\" -> 250K rows\n",
    "                           Key \"USA_3\" -> 250K rows\n",
    "```\n",
    "\n",
    "### When to Use Salting vs AQE\n",
    "\n",
    "- **Try AQE first** -- it handles moderate skew automatically.\n",
    "- **Use salting** when AQE is not enough (extreme skew, e.g., one key has 100x more\n",
    "  data than others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d37bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Detecting and fixing data skew --\n",
    "\n",
    "# Create a skewed dataset: department \"Engineering\" has many more rows.\n",
    "skewed_data = (\n",
    "    [(\"Engineering\", i * 1000) for i in range(100)] +   # 100 rows\n",
    "    [(\"Marketing\",   i * 1000) for i in range(10)]  +   # 10 rows\n",
    "    [(\"HR\",          i * 1000) for i in range(5)]    +   # 5 rows\n",
    "    [(\"Sales\",       i * 1000) for i in range(5)]        # 5 rows\n",
    ")\n",
    "df_skewed = spark.createDataFrame(skewed_data, [\"department\", \"revenue\"])\n",
    "\n",
    "# Step 1: DETECT skew -- look at the distribution\n",
    "print(\"=== Data distribution (skew detected!) ===\")\n",
    "df_skewed.groupBy(\"department\").count().orderBy(F.desc(\"count\")).show()\n",
    "\n",
    "# Step 2: SALTING -- add a random salt to spread the skewed key\n",
    "NUM_SALT_BUCKETS = 4\n",
    "\n",
    "df_salted = df_skewed.withColumn(\n",
    "    \"salt\", (F.rand() * NUM_SALT_BUCKETS).cast(\"int\")\n",
    ").withColumn(\n",
    "    \"salted_key\", F.concat(F.col(\"department\"), F.lit(\"_\"), F.col(\"salt\"))\n",
    ")\n",
    "\n",
    "# Aggregate on the salted key (distributes \"Engineering\" across 4 partitions)\n",
    "partial_agg = df_salted.groupBy(\"salted_key\", \"department\").agg(\n",
    "    F.sum(\"revenue\").alias(\"partial_sum\"),\n",
    "    F.count(\"*\").alias(\"partial_count\")\n",
    ")\n",
    "print(\"=== Partial aggregation on salted key ===\")\n",
    "partial_agg.show(truncate=False)\n",
    "\n",
    "# Remove salt and do final aggregation\n",
    "final_agg = partial_agg.groupBy(\"department\").agg(\n",
    "    F.sum(\"partial_sum\").alias(\"total_revenue\"),\n",
    "    F.sum(\"partial_count\").alias(\"total_count\")\n",
    ")\n",
    "print(\"=== Final aggregation after removing salt ===\")\n",
    "final_agg.orderBy(F.desc(\"total_revenue\")).show()\n",
    "\n",
    "# Verify: same result without salting\n",
    "print(\"=== Direct aggregation (for comparison) ===\")\n",
    "df_skewed.groupBy(\"department\").agg(\n",
    "    F.sum(\"revenue\").alias(\"total_revenue\"),\n",
    "    F.count(\"*\").alias(\"total_count\")\n",
    ").orderBy(F.desc(\"total_revenue\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26f184",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Best Practices Checklist -- Rules for Production Spark Code\n",
    "\n",
    "### Transformations and Queries\n",
    "\n",
    "| Rule | Why |\n",
    "|---|---|\n",
    "| Filter early, filter often | Reduces data volume before expensive operations |\n",
    "| Select only the columns you need | Column pruning saves memory and I/O |\n",
    "| Avoid `collect()` and `toPandas()` on large data | Pulls everything to the driver -- causes OOM |\n",
    "| Use built-in functions instead of UDFs | Built-in functions run in the JVM; UDFs serialize data to Python and back |\n",
    "| Use `F.col(\"x\")` instead of `df.x` | `F.col()` is unambiguous, especially after joins with duplicate column names |\n",
    "\n",
    "### Shuffle and Joins\n",
    "\n",
    "| Rule | Why |\n",
    "|---|---|\n",
    "| Broadcast small tables (`F.broadcast(df)`) | Eliminates shuffle in joins |\n",
    "| Use `coalesce(n)` to reduce partitions, not `repartition(n)` | Coalesce avoids a full shuffle |\n",
    "| Pre-partition data on join keys when writing | Future joins on the same key avoid a shuffle |\n",
    "| Avoid unnecessary `orderBy` | Global sort requires a shuffle; use it only when the consumer needs sorted output |\n",
    "\n",
    "### Caching\n",
    "\n",
    "| Rule | Why |\n",
    "|---|---|\n",
    "| Cache only if a DataFrame is used in multiple actions | Caching a one-time DataFrame wastes memory |\n",
    "| Always `unpersist()` when done | Frees cluster memory for other work |\n",
    "| Prefer Delta Cache over manual caching on Databricks | Delta Cache is automatic and disk-based |\n",
    "\n",
    "### Storage\n",
    "\n",
    "| Rule | Why |\n",
    "|---|---|\n",
    "| Use Delta format for all production tables | ACID transactions, time travel, MERGE support |\n",
    "| Partition on-disk by a column you filter on frequently (date, region) | Enables partition pruning |\n",
    "| Avoid too many partitions (small files problem) | Millions of tiny files slow down reads |\n",
    "| Use Z-ORDER for multi-column filter optimisation (Databricks) | Co-locates related data within files |\n",
    "\n",
    "### Debugging\n",
    "\n",
    "| Rule | Why |\n",
    "|---|---|\n",
    "| Use `explain(mode=\"formatted\")` to check plans | Catch unnecessary shuffles and missing pushdowns |\n",
    "| Check the Spark UI for skew (max vs median task time) | Skew is the number one cause of slow jobs |\n",
    "| Monitor `Exchange` nodes in the plan | Each Exchange is a shuffle -- minimise them |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b24e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Best practices applied: GOOD vs BAD examples --\n",
    "\n",
    "# ─── BAD: filter late, select all columns, unnecessary sort ─────────\n",
    "bad_query = (\n",
    "    df\n",
    "    .join(df_dept, on=\"department\")                   # join ALL rows first\n",
    "    .orderBy(\"salary\")                                 # unnecessary sort (adds shuffle)\n",
    "    .filter(F.col(\"department\") == \"Engineering\")     # filter AFTER join + sort\n",
    "    .select(\"name\", \"salary\")                         # select late\n",
    ")\n",
    "print(\"=== BAD query plan (filter late, unnecessary sort) ===\")\n",
    "bad_query.explain()\n",
    "\n",
    "# ─── GOOD: filter early, select only what is needed, broadcast ──────\n",
    "good_query = (\n",
    "    df\n",
    "    .filter(F.col(\"department\") == \"Engineering\")     # filter FIRST (reduces rows)\n",
    "    .select(\"name\", \"department\", \"salary\")           # select early (column pruning)\n",
    "    .join(F.broadcast(df_dept), on=\"department\")      # broadcast the small table\n",
    ")\n",
    "print(\"\\n=== GOOD query plan (filter early, broadcast, no unnecessary sort) ===\")\n",
    "good_query.explain()\n",
    "\n",
    "# Both produce the same result:\n",
    "print(\"\\n=== BAD result ===\")\n",
    "bad_query.show()\n",
    "print(\"=== GOOD result ===\")\n",
    "good_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94aabed",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary -- Concept Map\n",
    "\n",
    "```\n",
    "YOUR PYSPARK CODE\n",
    "       |\n",
    "       v\n",
    "  Transformations (lazy)          Actions (eager)\n",
    "  - select, filter, withColumn    - show, count, collect\n",
    "  - join, groupBy, orderBy        - write, toPandas, foreach\n",
    "       |                                |\n",
    "       v                                v\n",
    "  LOGICAL PLAN (what you want)    triggers EXECUTION\n",
    "       |\n",
    "       v  [Catalyst Optimizer]\n",
    "  OPTIMISED PLAN\n",
    "  - Predicate pushdown\n",
    "  - Column pruning\n",
    "  - Join reordering\n",
    "  - Constant folding\n",
    "       |\n",
    "       v\n",
    "  PHYSICAL PLAN\n",
    "  - Narrow transforms --> same partition (fast)\n",
    "  - Wide transforms   --> SHUFFLE (Exchange) (slow)\n",
    "       |\n",
    "       v\n",
    "  DAG --> Jobs --> Stages (split at shuffle) --> Tasks (one per partition)\n",
    "       |\n",
    "       v  [Tungsten Engine]\n",
    "  EXECUTION with code generation and memory management\n",
    "       |\n",
    "       v  [AQE -- runtime optimisation]\n",
    "  - Coalesce shuffle partitions\n",
    "  - Convert SortMergeJoin to BroadcastHashJoin\n",
    "  - Handle skew\n",
    "       |\n",
    "       v\n",
    "  RESULT (or written to Delta/Parquet)\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Lazy evaluation** lets Spark optimise before running -- do not fight it.\n",
    "2. **Transformations** build the plan; **actions** execute it.\n",
    "3. **Shuffle (Exchange)** is the most expensive operation -- minimise it.\n",
    "4. **Broadcast** small tables to eliminate shuffle in joins.\n",
    "5. **Cache** DataFrames you reuse in multiple actions; **unpersist** when done.\n",
    "6. **explain()** is your best debugging tool -- read plans bottom to top.\n",
    "7. **AQE** handles most runtime optimisations automatically on Databricks.\n",
    "8. **Salting** fixes extreme data skew that AQE cannot resolve.\n",
    "9. **Filter early, select only needed columns, use built-in functions over UDFs.**\n",
    "10. **Use Delta format** for production storage -- it adds ACID, time travel, and MERGE.\n",
    "\n",
    "---\n",
    "*This notebook is compatible with Databricks Community Edition (Free) and Databricks Serverless Compute.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
